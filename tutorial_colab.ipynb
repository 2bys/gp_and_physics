{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS FOR CHAPTER 1\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "# ---------------------------------\n",
    "# % Animation of damped oscillator\n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "def animate_damped_oscillator(\n",
    "    damped_oscillator_fn,\n",
    "    t_anim,\n",
    "    theta,\n",
    "    fps=24,\n",
    "    n_coils=12,\n",
    "    amp=0.11,\n",
    "    trace_len=200,\n",
    "):\n",
    "    y_anim = damped_oscillator_fn(t_anim, theta)\n",
    "    t_anim_np = np.asarray(t_anim)\n",
    "    y_anim_np = np.asarray(y_anim)\n",
    "\n",
    "    def _spring_points(\n",
    "        x_left: float, x_right: float, n_coils: int = n_coils, amp: float = amp\n",
    "    ):\n",
    "        xs = np.linspace(x_left, x_right, n_coils * 2 + 1)\n",
    "        ys = np.zeros_like(xs)\n",
    "        ys[1:-1:2] = amp\n",
    "        ys[2:-1:2] = -amp\n",
    "        ys[0] = ys[-1] = 0.0\n",
    "        return xs, ys\n",
    "\n",
    "    fig, (ax_left, ax_right) = plt.subplots(\n",
    "        1, 2, figsize=(12, 3), gridspec_kw={\"width_ratios\": [1, 1.2]}\n",
    "    )\n",
    "    fig.suptitle(\"Damped Harmonic Oscillator — Mass Motion & Displacement\", fontsize=14)\n",
    "\n",
    "    A = theta.A\n",
    "    ax_left.set_xlim(-A * 1.8 - 0.4, A * 1.8 + 0.4)\n",
    "    ax_left.set_ylim(-0.8, 0.8)\n",
    "    ax_left.set_xlabel(\"Position\")\n",
    "    ax_left.set_yticks([])\n",
    "    ax_left.grid(True, axis=\"x\")\n",
    "    wall_x = -A * 1.8 - 0.2\n",
    "    ax_left.plot([wall_x, wall_x], [-0.8, 0.8], linewidth=4)\n",
    "\n",
    "    mass_w, mass_h = 0.28, 0.32\n",
    "    mass = plt.Rectangle(\n",
    "        (y_anim_np[0] - mass_w / 2, -mass_h / 2), mass_w, mass_h, ec=\"black\", fc=\"C0\"\n",
    "    )\n",
    "    ax_left.add_patch(mass)\n",
    "    (spring_line,) = ax_left.plot([], [], lw=2)\n",
    "    (trace_line,) = ax_left.plot([], [], lw=1, alpha=0.6)\n",
    "    trace_buf = []\n",
    "\n",
    "    ax_right.set_xlim(t_anim_np[0], t_anim_np[-1])\n",
    "    ax_right.set_ylim(-A * 1.2, A * 1.2)\n",
    "    ax_right.set_xlabel(\"Time t\")\n",
    "    ax_right.set_ylabel(\"Displacement y(t)\")\n",
    "    ax_right.grid(True)\n",
    "    ax_right.plot(t_anim_np, y_anim_np, alpha=0.4, label=\"True displacement\")\n",
    "    (marker_dot,) = ax_right.plot([], [], \"o\", label=\"Current position\")\n",
    "    y0, y1 = ax_right.get_ylim()\n",
    "    (time_line,) = ax_right.plot(\n",
    "        [t_anim_np[0], t_anim_np[0]], [y0, y1], linestyle=\"--\", alpha=0.6\n",
    "    )\n",
    "    ax_right.legend(loc=\"upper right\")\n",
    "\n",
    "    def init():\n",
    "        spring_line.set_data([], [])\n",
    "        trace_line.set_data([], [])\n",
    "        marker_dot.set_data([], [])\n",
    "        y0, y1 = ax_right.get_ylim()\n",
    "        time_line.set_data([t_anim_np[0], t_anim_np[0]], [y0, y1])\n",
    "        return mass, spring_line, trace_line, marker_dot, time_line\n",
    "\n",
    "    def animate(i):\n",
    "        x_pos = y_anim_np[i]\n",
    "        mass.set_xy((x_pos - mass_w / 2, -mass_h / 2))\n",
    "        xs, ys = _spring_points(wall_x, x_pos - mass_w / 2)\n",
    "        spring_line.set_data(xs, ys)\n",
    "        trace_buf.append(x_pos)\n",
    "        if len(trace_buf) > trace_len:\n",
    "            del trace_buf[0]\n",
    "        trace_line.set_data(np.array(trace_buf), np.zeros(len(trace_buf)))\n",
    "        marker_dot.set_data([t_anim_np[i]], [y_anim_np[i]])\n",
    "        y0, y1 = ax_right.get_ylim()\n",
    "        time_line.set_data([t_anim_np[i], t_anim_np[i]], [y0, y1])\n",
    "        return mass, spring_line, trace_line, marker_dot, time_line\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig,\n",
    "        animate,\n",
    "        init_func=init,\n",
    "        frames=len(t_anim_np),\n",
    "        interval=1000 / fps,\n",
    "        blit=True,\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    return HTML(anim.to_jshtml())\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# % Plotting GPs and interactive widgets\n",
    "# ---------------------------------------\n",
    "\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "def plot_gp(\n",
    "    ax,\n",
    "    gp_or_dist,\n",
    "    X_grid,\n",
    "    *,\n",
    "    n_samples: int = 0,\n",
    "    seed: int = 0,\n",
    "    mean_label: str = \"Mean\",\n",
    "    shade_k: float = 2.0,\n",
    "    shade_label: str = \"\\pm2\\sigma\",\n",
    "    true_curve=None,  # (X_true, y_true)\n",
    "    obs=None,  # (X_obs, y_obs)\n",
    "    obs_style=None,\n",
    "    sample_style=None,\n",
    "    mean_style=None,\n",
    "    band_alpha: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws GP mean, ±ksigma band, optional samples, optional true curve, and observations.\n",
    "    Works with either a GP (callable returning a dist) or a distribution directly.\n",
    "    \"\"\"\n",
    "    # Get a distribution at X_grid\n",
    "    if hasattr(gp_or_dist, \"mu\") and hasattr(gp_or_dist, \"Sigma\"):\n",
    "        dist = gp_or_dist\n",
    "    else:\n",
    "        dist = gp_or_dist(X_grid)  # assume GP-like callable\n",
    "\n",
    "    Xr = np.asarray(X_grid).ravel()\n",
    "    mu = np.asarray(dist.mu).reshape(-1)\n",
    "    std = np.sqrt(np.asarray(dist.Sigma).diagonal())\n",
    "\n",
    "    # Styles\n",
    "    if mean_style is None:\n",
    "        mean_style = dict(lw=2, label=mean_label)\n",
    "    if sample_style is None:\n",
    "        sample_style = dict(lw=1.0, alpha=0.8)\n",
    "    if obs_style is None:\n",
    "        obs_style = dict(s=60, marker=\"x\", color=\"red\", zorder=5, label=\"Observed\")\n",
    "\n",
    "    # Plot uncertainty band + mean\n",
    "    ax.fill_between(\n",
    "        Xr, mu - shade_k * std, mu + shade_k * std, alpha=band_alpha, label=shade_label\n",
    "    )\n",
    "    ax.plot(Xr, mu, **mean_style)\n",
    "\n",
    "    # Optional samples\n",
    "    if n_samples > 0:\n",
    "        S = dist.sample(jax.random.PRNGKey(seed), num_samples=n_samples)\n",
    "        ax.plot(Xr, np.asarray(S).T, **sample_style)\n",
    "\n",
    "    # Optional ground truth\n",
    "    if true_curve is not None:\n",
    "        X_true, y_true = true_curve\n",
    "        X_true, y_true = np.asarray(X_true).ravel(), np.asarray(y_true).ravel()\n",
    "        ax.plot(\n",
    "            X_true,\n",
    "            y_true,\n",
    "            lw=2,\n",
    "            alpha=0.7,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            label=\"True Signal\",\n",
    "        )\n",
    "\n",
    "    # Optional observations\n",
    "    if obs is not None:\n",
    "        X_obs, y_obs = obs\n",
    "        X_obs, y_obs = np.asarray(X_obs).ravel(), np.asarray(y_obs).ravel()\n",
    "        ax.scatter(X_obs, y_obs, **obs_style)\n",
    "\n",
    "    return mu, std  # handy for metrics etc.\n",
    "\n",
    "\n",
    "class InteractiveGPPlotter:\n",
    "    def __init__(\n",
    "        self,\n",
    "        plot_type: str,\n",
    "        *,\n",
    "        kernel_registry,\n",
    "        gp_cls=None,\n",
    "        mean_fn=None,\n",
    "        training_data=None,\n",
    "        test_data=None,\n",
    "    ):\n",
    "        assert plot_type in {\"kernel\", \"prior\", \"posterior\"}\n",
    "        self.plot_type = plot_type\n",
    "        self.kernel_registry = kernel_registry\n",
    "        self.gp_cls = gp_cls\n",
    "        self.mean_fn = mean_fn\n",
    "\n",
    "        self.X_train, self.y_train = training_data if training_data else (None, None)\n",
    "        self.X_test, self.y_test = test_data if test_data else (None, None)\n",
    "\n",
    "        if self.plot_type == \"posterior\":\n",
    "            if any(\n",
    "                v is None\n",
    "                for v in (self.X_train, self.y_train, self.X_test, self.y_test)\n",
    "            ):\n",
    "                raise ValueError(\"posterior plot requires training_data and test_data.\")\n",
    "\n",
    "        self.widgets = self._create_widgets()\n",
    "        self.controls = self._layout_widgets()\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(9, 4.5))\n",
    "        self.ui = widgets.VBox([self.controls, self.fig.canvas])\n",
    "\n",
    "        self._link_widgets_to_redraw()\n",
    "        self.redraw()\n",
    "\n",
    "    def _create_widgets(self) -> dict[str, widgets.Widget]:\n",
    "        style = {\"description_width\": \"110px\"}\n",
    "        w = {\n",
    "            \"kernel\": widgets.Dropdown(\n",
    "                options=list(self.kernel_registry.keys()),\n",
    "                value=next(iter(self.kernel_registry.keys())),\n",
    "                description=\"Kernel\",\n",
    "                style=style,\n",
    "            ),\n",
    "            \"variance\": widgets.FloatLogSlider(\n",
    "                value=1.0, base=10, min=-2, max=2, description=\"Variance\", style=style\n",
    "            ),\n",
    "            \"lengthscale\": widgets.FloatLogSlider(\n",
    "                value=1.0,\n",
    "                base=10,\n",
    "                min=-2,\n",
    "                max=2,\n",
    "                description=\"Lengthscale\",\n",
    "                style=style,\n",
    "            ),\n",
    "            \"period\": widgets.FloatSlider(\n",
    "                value=np.pi,\n",
    "                min=0.2,\n",
    "                max=8.0,\n",
    "                step=0.05,\n",
    "                description=\"Period\",\n",
    "                style=style,\n",
    "            ),\n",
    "        }\n",
    "        if self.plot_type == \"kernel\":\n",
    "            w[\"xmax\"] = widgets.FloatSlider(\n",
    "                value=5.0, min=1.0, max=15, step=0.1, description=\"x_max\", style=style\n",
    "            )\n",
    "        elif self.plot_type == \"prior\":\n",
    "            w.update(\n",
    "                {\n",
    "                    \"samples\": widgets.IntSlider(\n",
    "                        value=5, min=0, max=30, description=\"# Samples\", style=style\n",
    "                    ),\n",
    "                    \"seed\": widgets.IntSlider(\n",
    "                        value=0, min=0, max=100, description=\"Seed\", style=style\n",
    "                    ),\n",
    "                    \"xmin\": widgets.FloatSlider(\n",
    "                        value=-5.0,\n",
    "                        min=-15,\n",
    "                        max=0,\n",
    "                        step=0.1,\n",
    "                        description=\"x_min\",\n",
    "                        style=style,\n",
    "                    ),\n",
    "                    \"xmax\": widgets.FloatSlider(\n",
    "                        value=5.0,\n",
    "                        min=0,\n",
    "                        max=15,\n",
    "                        step=0.1,\n",
    "                        description=\"x_max\",\n",
    "                        style=style,\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            w.update(\n",
    "                {\n",
    "                    \"noise\": widgets.FloatLogSlider(\n",
    "                        value=0.1,\n",
    "                        base=10,\n",
    "                        min=-3,\n",
    "                        max=0,\n",
    "                        description=\"σ_noise\",\n",
    "                        style=style,\n",
    "                    ),\n",
    "                    \"samples\": widgets.IntSlider(\n",
    "                        value=3, min=0, max=30, description=\"# Samples\", style=style\n",
    "                    ),\n",
    "                    \"seed\": widgets.IntSlider(\n",
    "                        value=0, min=0, max=100, description=\"Seed\", style=style\n",
    "                    ),\n",
    "                    \"n_used\": widgets.IntSlider(\n",
    "                        value=min(10, len(self.X_train)),\n",
    "                        min=0,\n",
    "                        max=len(self.X_train),\n",
    "                        description=\"# Points\",\n",
    "                        style=style,\n",
    "                    ),\n",
    "                    \"metrics\": widgets.HTML(\n",
    "                        value=\"<pre style='margin:0'>RMSE: –\\nMLPD: –</pre>\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "        return w\n",
    "\n",
    "    def _layout_widgets(self) -> widgets.Widget:\n",
    "        w = self.widgets\n",
    "        kernel_box = widgets.VBox(\n",
    "            [w[\"kernel\"], w[\"variance\"], w[\"lengthscale\"], w[\"period\"]]\n",
    "        )\n",
    "        if self.plot_type == \"kernel\":\n",
    "            return widgets.HBox([kernel_box, widgets.VBox([w[\"xmax\"]])])\n",
    "        if self.plot_type == \"prior\":\n",
    "            sampling_box = widgets.VBox([w[\"samples\"], w[\"xmin\"], w[\"xmax\"], w[\"seed\"]])\n",
    "            return widgets.HBox([kernel_box, sampling_box])\n",
    "        data_box = widgets.VBox([w[\"noise\"], w[\"n_used\"], w[\"samples\"], w[\"seed\"]])\n",
    "        metrics_box = widgets.VBox([widgets.HTML(\"<b>Metrics</b>\"), w[\"metrics\"]])\n",
    "        return widgets.HBox([kernel_box, data_box, metrics_box])\n",
    "\n",
    "    def _link_widgets_to_redraw(self):\n",
    "        for name, widget in self.widgets.items():\n",
    "            if name == \"metrics\":\n",
    "                continue\n",
    "            widget.observe(self.redraw, names=\"value\")\n",
    "\n",
    "    def _get_kernel(self):\n",
    "        KernelClass = self.kernel_registry[self.widgets[\"kernel\"].value]\n",
    "        params = {\n",
    "            \"variance\": float(self.widgets[\"variance\"].value),\n",
    "            \"lengthscale\": float(self.widgets[\"lengthscale\"].value),\n",
    "            \"period\": float(self.widgets[\"period\"].value),\n",
    "        }\n",
    "        # show/hide period control if the kernel suggests it\n",
    "        show_period = (\n",
    "            \"period\" in KernelClass.__name__.lower()\n",
    "            or \"periodic\" in self.widgets[\"kernel\"].value.lower()\n",
    "        )\n",
    "        self.widgets[\"period\"].layout.display = \"\" if show_period else \"none\"\n",
    "        return KernelClass.from_params(params)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_metrics(mu, std, y_true):\n",
    "        eps = 1e-12\n",
    "        rmse = float(np.sqrt(np.mean((mu - y_true) ** 2)))\n",
    "        var = std**2 + eps\n",
    "        mlpd = float(\n",
    "            np.mean(-0.5 * np.log(2 * np.pi * var) - 0.5 * ((y_true - mu) ** 2) / var)\n",
    "        )\n",
    "        return rmse, mlpd\n",
    "\n",
    "    # --- redraw\n",
    "    def redraw(self, *_):\n",
    "        ax = self.ax\n",
    "        ax.cla()\n",
    "\n",
    "        if self.plot_type == \"kernel\":\n",
    "            self._plot_kernel(ax)\n",
    "        elif self.plot_type == \"prior\":\n",
    "            self._plot_prior(ax)\n",
    "        else:\n",
    "            self._plot_posterior(ax)\n",
    "\n",
    "        ax.grid(True, alpha=0.5)\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        self.fig.canvas.draw_idle()\n",
    "\n",
    "    # --- individual plot modes\n",
    "    def _plot_kernel(self, ax: plt.Axes):\n",
    "        w = self.widgets\n",
    "        kernel = self._get_kernel()\n",
    "        xmax = float(w[\"xmax\"].value)\n",
    "        X = jnp.linspace(-xmax, xmax, 500)[:, None]\n",
    "        ax.plot(X, kernel(X, jnp.array([[0.0]])), lw=2, label=\"k(x, 0)\")\n",
    "        ax.set(\n",
    "            ylim=(-0.1, float(w[\"variance\"].value) * 1.1),\n",
    "            title=f\"'{w['kernel'].value}' Kernel Function\",\n",
    "            xlabel=\"x\",\n",
    "            ylabel=\"k(x,0)\",\n",
    "        )\n",
    "\n",
    "    def _plot_prior(self, ax: plt.Axes):\n",
    "        w = self.widgets\n",
    "        kernel = self._get_kernel()\n",
    "        gp = self.gp_cls(m=self.mean_fn, k=kernel)\n",
    "        X_grid = jnp.linspace(float(w[\"xmin\"].value), float(w[\"xmax\"].value), 400)[\n",
    "            :, None\n",
    "        ]\n",
    "        mu, std = plot_gp(\n",
    "            ax,\n",
    "            gp,\n",
    "            X_grid,\n",
    "            n_samples=int(w[\"samples\"].value),\n",
    "            seed=int(w[\"seed\"].value),\n",
    "            mean_label=\"Prior Mean\",\n",
    "            shade_k=2.0,\n",
    "            shade_label=\"±2σ\",\n",
    "        )\n",
    "        ax.set(title=f\"GP Prior — {w['kernel'].value}\", xlabel=\"x\", ylabel=\"f(x)\")\n",
    "\n",
    "    def _plot_posterior(self, ax: plt.Axes):\n",
    "        w = self.widgets\n",
    "        kernel = self._get_kernel()\n",
    "        gp = self.gp_cls(m=self.mean_fn, k=kernel)\n",
    "\n",
    "        X_grid = jnp.linspace(float(self.X_test.min()), float(self.X_test.max()), 400)[\n",
    "            :, None\n",
    "        ]\n",
    "        dist = gp(X_grid)\n",
    "\n",
    "        n = int(w[\"n_used\"].value)\n",
    "        obs = None\n",
    "        if n > 0:\n",
    "            X_obs = self.X_train[:n]\n",
    "            y_obs = self.y_train[:n]\n",
    "\n",
    "            gp = gp.condition(\n",
    "                y=jnp.asarray(y_obs),\n",
    "                X=jnp.asarray(X_obs),\n",
    "                sigma2=float(w[\"noise\"].value) ** 2,\n",
    "            )\n",
    "            dist = gp(X_grid)\n",
    "            obs = (X_obs, y_obs)\n",
    "\n",
    "        mu, std = plot_gp(\n",
    "            ax,\n",
    "            dist,\n",
    "            X_grid,\n",
    "            n_samples=int(w[\"samples\"].value),\n",
    "            seed=int(w[\"seed\"].value),\n",
    "            mean_label=\"Posterior Mean\",\n",
    "            true_curve=(self.X_test, self.y_test),\n",
    "            obs=obs,\n",
    "        )\n",
    "        # metrics\n",
    "        y_interp = np.interp(\n",
    "            np.asarray(X_grid).ravel(), self.X_test.ravel(), self.y_test.ravel()\n",
    "        )\n",
    "        rmse, mlpd = self._compute_metrics(mu, std, y_interp)\n",
    "        self.widgets[\n",
    "            \"metrics\"\n",
    "        ].value = f\"<pre style='margin:0'>RMSE: {rmse:.4f}\\nMLPD: {mlpd:.4f}</pre>\"\n",
    "        ax.set(title=f\"GP Posterior — {w['kernel'].value}\", xlabel=\"x\", ylabel=\"f(x)\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Optimization widget\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "class OptimizationWidget:\n",
    "    \"\"\"Manages the UI for fitting and plotting.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tuner,\n",
    "        kernel_registry: dict,\n",
    "        param_init_registry: dict,\n",
    "        test_data: tuple,\n",
    "        gp_cls,\n",
    "        extrapolate: float | bool = False,\n",
    "    ):\n",
    "        self.tuner = tuner\n",
    "        self.gp_cls = gp_cls\n",
    "        self.kernel_registry = kernel_registry\n",
    "        self.param_init_registry = param_init_registry\n",
    "        self.X_test, self.y_test = test_data\n",
    "\n",
    "        self.kernel_sel = widgets.Dropdown(\n",
    "            options=list(self.kernel_registry.keys()), description=\"Kernel\"\n",
    "        )\n",
    "        self.fit_button = widgets.Button(\n",
    "            description=\"Fit Hyperparameters\", button_style=\"success\"\n",
    "        )\n",
    "\n",
    "        self.fig, self.ax = plt.subplots(figsize=(9, 4.5))\n",
    "        self.ui = widgets.VBox(\n",
    "            [widgets.HBox([self.kernel_sel, self.fit_button]), self.fig.canvas]\n",
    "        )\n",
    "        self.fit_button.on_click(self._on_fit_clicked)\n",
    "\n",
    "        # Initial empty plot\n",
    "        self.ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Select a kernel and click 'Fit' to begin.\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "        )\n",
    "        self.ax.grid(True)\n",
    "\n",
    "        self.extrapolate = extrapolate\n",
    "\n",
    "    def _format_results_text(self, kernel_name, fit_results):\n",
    "        \"\"\"Formats the optimization results into a string for plotting.\"\"\"\n",
    "        lines = [f\"--- Optimized: {kernel_name} ---\"]\n",
    "        final_params = fit_results[\"final_params\"]\n",
    "        log_params = fit_results[\"optimized_params_log\"]\n",
    "\n",
    "        for key, val in final_params.items():\n",
    "            if key in [\"noise\", \"sigma2\"]:\n",
    "                continue\n",
    "            log_key = \"log_\" + key if key in [\"variance\", \"lengthscale\"] else key\n",
    "            log_val_str = (\n",
    "                f\"(log={log_params.get(log_key, 'N/A'):.2f})\"\n",
    "                if log_key in log_params\n",
    "                else \"\"\n",
    "            )\n",
    "            lines.append(f\"{key:<12s} = {val:.3f} {log_val_str}\")\n",
    "\n",
    "        lines.append(\n",
    "            f\"{'noise':<12s} = {final_params['noise']:.3f}  (log={log_params['log_noise']:.2f})\"\n",
    "        )\n",
    "        lines.append(f\"Final Neg MLL = {fit_results['result'].fun:.2f}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _on_fit_clicked(self, _):\n",
    "        # 1. Update the plot to show an \"Optimizing...\" message\n",
    "        self.ax.cla()\n",
    "        self.ax.text(0.5, 0.5, \"Optimizing...\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "        self.fig.canvas.draw_idle()\n",
    "\n",
    "        # 2. Run the optimization\n",
    "        kernel_name = self.kernel_sel.value\n",
    "        fit_results = self.tuner.fit(\n",
    "            kernel_name, self.kernel_registry, self.param_init_registry\n",
    "        )\n",
    "\n",
    "        # 3. Build the optimized GP posterior\n",
    "        final_params = fit_results[\"final_params\"]\n",
    "        sigma2_opt = final_params[\"sigma2\"]\n",
    "        k_opt = self.kernel_registry[kernel_name].from_params(final_params)\n",
    "\n",
    "        gp_prior_opt = self.gp_cls(m=lambda x: jnp.zeros(x.shape[0]), k=k_opt)\n",
    "        post_gp = gp_prior_opt.condition(\n",
    "            y=self.tuner.y, X=self.tuner.X, sigma2=sigma2_opt\n",
    "        )\n",
    "\n",
    "        # 4. Clear the axes and draw the final plot\n",
    "        self.ax.cla()\n",
    "        X_grid = (\n",
    "            jnp.linspace(\n",
    "                self.X_test.min() - self.extrapolate,\n",
    "                self.X_test.max() + self.extrapolate,\n",
    "                500,\n",
    "            )[:, None]\n",
    "            if self.extrapolate\n",
    "            else self.X_test[:, None]\n",
    "        )\n",
    "        plot_gp(\n",
    "            self.ax,\n",
    "            post_gp,\n",
    "            X_grid,\n",
    "            n_samples=3,\n",
    "            obs=(self.tuner.X, self.tuner.y),\n",
    "            true_curve=(self.X_test, self.y_test),\n",
    "        )\n",
    "        self.ax.set_title(f\"Posterior with Optimized '{kernel_name}' Kernel\")\n",
    "\n",
    "        # 5. Add the formatted results text to the plot\n",
    "        results_text = self._format_results_text(kernel_name, fit_results)\n",
    "        self.ax.text(\n",
    "            0.02,\n",
    "            0.98,\n",
    "            results_text,\n",
    "            transform=self.ax.transAxes,\n",
    "            verticalalignment=\"top\",\n",
    "            fontfamily=\"monospace\",\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"wheat\", alpha=0.7),\n",
    "        )\n",
    "\n",
    "        self.ax.legend(loc=\"upper right\")\n",
    "        self.ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.integrate as spi\n",
    "from scipy.interpolate import interp1d\n",
    "from tueplots.constants.color import palettes\n",
    "\n",
    "tue_col_1 = palettes.tue_plot[0]\n",
    "tue_col_2 = palettes.tue_plot[1]\n",
    "\n",
    "# ----------------------------------\n",
    "# CPU Problem\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CPUHeatProblem1D:\n",
    "    \"\"\"\n",
    "    Encapsulates the 1D CPU heat problem, now independent of linpde_gp.\n",
    "    The ground truth solution is computed numerically.\n",
    "    \"\"\"\n",
    "\n",
    "    # Geometry\n",
    "    width: float\n",
    "    domain: tuple[float, float]\n",
    "\n",
    "    # Material & Heat Properties\n",
    "    kappa: float\n",
    "    TDP: float\n",
    "\n",
    "    # Ground Truth Solution (numerically computed)\n",
    "    solution: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "    # PDE right-hand-side function\n",
    "    q_total: Callable[[np.ndarray], np.ndarray]\n",
    "\n",
    "    # Synthetic DTS sensor data\n",
    "    X_dts: np.ndarray\n",
    "    y_dts: np.ndarray\n",
    "    dts_noise_std: float\n",
    "\n",
    "\n",
    "def create_cpu_problem() -> CPUHeatProblem1D:\n",
    "    \"\"\"\n",
    "    Factory function to set up the CPU heat problem using NumPy and SciPy.\n",
    "    \"\"\"\n",
    "    # -- Geometry\n",
    "    width, height, depth = 16.28, 9.19, 0.37\n",
    "    domain = (0.0, width)\n",
    "    V = width * height * depth\n",
    "\n",
    "    # -- Material property\n",
    "    kappa = 1.56 * 10.0\n",
    "\n",
    "    # -- Heat sources\n",
    "    TDP = 95.0\n",
    "    N_cores_x, core_width, core_offset_x, core_distance_x = 3, 2.5, 1.95, 0.35\n",
    "    core_centers_xs = (\n",
    "        core_offset_x\n",
    "        + (core_width + core_distance_x) * np.arange(N_cores_x, dtype=np.double)\n",
    "        + core_width / 2.0\n",
    "    )\n",
    "\n",
    "    # -- Heat source function (re-implemented with NumPy)\n",
    "    def create_core_heat_source_fn(rel_heights=[0.9, 0.75, 1.0]):\n",
    "        \"\"\"Creates a callable heat source function using interpolation.\"\"\"\n",
    "        xs, ys = [domain[0]], [0.0]\n",
    "        eps = core_distance_x / 3\n",
    "        for cx, h in zip(core_centers_xs, rel_heights):\n",
    "            xs.extend(\n",
    "                [\n",
    "                    cx - core_width / 2 - eps,\n",
    "                    cx - core_width / 2,\n",
    "                    cx + core_width / 2,\n",
    "                    cx + core_width / 2 + eps,\n",
    "                ]\n",
    "            )\n",
    "            ys.extend([0.0, h, h, 0.0])\n",
    "        xs.append(domain[1])\n",
    "        ys.append(0.0)\n",
    "\n",
    "        # Normalize the distribution using the trapezoidal rule\n",
    "        norm_const = np.trapz(ys, xs)\n",
    "        ys_normalized = np.array(ys) / norm_const\n",
    "\n",
    "        # Return a callable interpolation function\n",
    "        return interp1d(xs, ys_normalized, bounds_error=False, fill_value=0.0)\n",
    "\n",
    "    q_src_dist = create_core_heat_source_fn()\n",
    "\n",
    "    def q_dot_V_src(x):\n",
    "        return (TDP / (depth * height)) * q_src_dist(x)\n",
    "\n",
    "    q_dot_V_sink = -TDP / V\n",
    "\n",
    "    def q_total(x):\n",
    "        return q_dot_V_src(x) + q_dot_V_sink\n",
    "\n",
    "    # -- Numerical Ground Truth Solution --\n",
    "    def create_numerical_solution(u0=60.0, du0=0.0):\n",
    "        \"\"\"Solves -k u'' = q_total for u using numerical integration.\"\"\"\n",
    "        x_dense = np.linspace(domain[0], domain[1], 2001)\n",
    "\n",
    "        # We solve u'' = -q_total(x) / kappa\n",
    "        rhs = -q_total(x_dense) / kappa\n",
    "        du_dx = spi.cumulative_trapezoid(rhs, x_dense, initial=0.0) + du0\n",
    "        u_values = spi.cumulative_trapezoid(du_dx, x_dense, initial=0.0) + u0\n",
    "\n",
    "        # Return a callable interpolation function\n",
    "        return interp1d(\n",
    "            x_dense,\n",
    "            u_values,\n",
    "            bounds_error=False,\n",
    "            fill_value=(u_values[0], u_values[-1]),\n",
    "        )\n",
    "\n",
    "    solution_fn = create_numerical_solution(u0=60.0, du0=0.0)\n",
    "\n",
    "    # -- Synthetic DTS Data --\n",
    "    dts_noise_std = 0.5\n",
    "    noise_rng = np.random.default_rng(33215)\n",
    "    noise_dts = noise_rng.normal(scale=dts_noise_std, size=len(core_centers_xs))\n",
    "    y_dts = solution_fn(core_centers_xs) + noise_dts\n",
    "\n",
    "    return CPUHeatProblem1D(\n",
    "        width=width,\n",
    "        domain=domain,\n",
    "        kappa=kappa,\n",
    "        TDP=TDP,\n",
    "        solution=solution_fn,\n",
    "        q_total=q_total,\n",
    "        X_dts=core_centers_xs,\n",
    "        y_dts=y_dts,\n",
    "        dts_noise_std=dts_noise_std,\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting utilities\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def plot_gp_belief_and_pde(\n",
    "    gp,\n",
    "    problem: CPUHeatProblem1D,\n",
    "    X_grid: np.ndarray,\n",
    "    conditions: list | None = None,\n",
    "    n_samples: int = 0,\n",
    "    seed: int = 0,\n",
    "    title: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a two-panel plot, now with support for visualizing integral conditions.\n",
    "    \"\"\"\n",
    "    fig, (ax_u, ax_pde) = plt.subplots(\n",
    "        ncols=2, sharex=True, figsize=(12, 3.5), constrained_layout=True\n",
    "    )\n",
    "    if conditions is None:\n",
    "        conditions = []\n",
    "\n",
    "    # -- Prepare GP data for plotting --\n",
    "    X_plot = np.asarray(X_grid).reshape(-1, 1)\n",
    "    gp_eval = gp(jnp.asarray(X_plot))\n",
    "    mu = np.asarray(gp_eval.mu).flatten()\n",
    "    Sigma = np.asarray(gp_eval.Sigma)\n",
    "    std = np.sqrt(Sigma.diagonal())\n",
    "\n",
    "    # --- 1. Left Panel: Belief over Temperature u(x) ---\n",
    "    ax_u.set(\n",
    "        ylabel=\"Temperature (°C)\",\n",
    "        title=\"Belief over Temperature $u(x)$\",\n",
    "        xlim=problem.domain,\n",
    "    )\n",
    "    ax_u.grid(True)\n",
    "    ax_u.plot(\n",
    "        X_grid, problem.solution(X_grid), color=\"k\", lw=2, ls=\"-\", label=\"True Solution\"\n",
    "    )\n",
    "    ax_u.fill_between(\n",
    "        X_grid,\n",
    "        mu - 1.96 * std,\n",
    "        mu + 1.96 * std,\n",
    "        color=\"C0\",\n",
    "        alpha=0.2,\n",
    "        label=\"95% Credible Interval\",\n",
    "    )\n",
    "    ax_u.plot(X_grid, mu, color=\"C0\", lw=2, label=\"GP Mean\")\n",
    "    if n_samples > 0:\n",
    "        samples = gp_eval.sample(jax.random.key(seed), n_samples)\n",
    "        ax_u.plot(X_grid, samples.T, color=\"C0\", lw=1.0, alpha=0.5)\n",
    "\n",
    "    # --- 2. Right Panel: PDE Balance Check ---\n",
    "    ax_pde.set(\n",
    "        xlabel=\"x-position (mm)\",\n",
    "        ylabel=\"Heat Flow\",\n",
    "        title=r\"PDE Balance: $-\\kappa u''(x)$ vs $\\dot{q}_V(x)$\",\n",
    "    )\n",
    "    ax_pde.grid(True)\n",
    "    ax_pde.plot(\n",
    "        X_grid,\n",
    "        problem.q_total(X_grid),\n",
    "        color=\"C1\",\n",
    "        lw=2,\n",
    "        label=r\"Heat Source $\\dot{q}_V(x)$\",\n",
    "    )\n",
    "\n",
    "    h = np.median(np.diff(X_grid))\n",
    "    d2_mu = (np.roll(mu, -1) - 2 * mu + np.roll(mu, 1)) / h**2\n",
    "    d2_mu[[0, -1]] = d2_mu[[1, -2]]\n",
    "    mean_Lu = -problem.kappa * d2_mu\n",
    "    N = len(X_grid)\n",
    "    D = np.zeros((N, N))\n",
    "    D[range(1, N - 1), range(0, N - 2)] = 1\n",
    "    D[range(1, N - 1), range(1, N - 1)] = -2\n",
    "    D[range(1, N - 1), range(2, N)] = 1\n",
    "    D /= h**2\n",
    "    D[0, 0:3] = [1, -2, 1]\n",
    "    D[-1, -3:] = [1, -2, 1]\n",
    "    Sigma_d2 = D @ Sigma @ D.T\n",
    "    var_d2 = np.diag(Sigma_d2)\n",
    "    std_Lu = problem.kappa * np.sqrt(np.maximum(var_d2, 0))\n",
    "    ax_pde.plot(X_grid, mean_Lu, color=\"C0\", lw=2, label=r\"GP-implied $-\\kappa u''(x)$\")\n",
    "    ax_pde.fill_between(\n",
    "        X_grid, mean_Lu - 1.96 * std_Lu, mean_Lu + 1.96 * std_Lu, color=\"C0\", alpha=0.2\n",
    "    )\n",
    "\n",
    "    if n_samples > 0:\n",
    "        d2_samples = np.array(\n",
    "            [\n",
    "                (np.roll(sample, -1) - 2 * sample + np.roll(sample, 1)) / h**2\n",
    "                for sample in samples\n",
    "            ]\n",
    "        )\n",
    "        d2_samples[:, [0, -1]] = d2_samples[:, [1, -2]]\n",
    "        Lu_samples = -problem.kappa * d2_samples\n",
    "        ax_pde.plot(\n",
    "            X_grid,\n",
    "            Lu_samples.T,\n",
    "            color=\"C0\",\n",
    "            lw=1.0,\n",
    "            alpha=0.5,\n",
    "        )\n",
    "\n",
    "    # -- Plot all provided conditions --\n",
    "    for cond in conditions:\n",
    "        cond_name = type(cond.op).__name__\n",
    "        if cond_name == \"PDEObservation\":\n",
    "            ax_pde.scatter(\n",
    "                cond.X,\n",
    "                cond.y_vec,\n",
    "                s=40,\n",
    "                marker=\"x\",\n",
    "                color=tue_col_1,\n",
    "                label=\"PDE Collocation Points\",\n",
    "                zorder=5,\n",
    "            )\n",
    "        elif cond_name == \"SensorObservation\":\n",
    "            ax_u.errorbar(\n",
    "                cond.X,\n",
    "                cond.y_vec,\n",
    "                yerr=np.sqrt(cond.op.sigma2),\n",
    "                fmt=\"o\",\n",
    "                color=tue_col_2,\n",
    "                label=\"Sensor Observations\",\n",
    "                zorder=5,\n",
    "            )\n",
    "            ax_pde.scatter(\n",
    "                cond.X,\n",
    "                np.zeros_like(cond.X),\n",
    "                marker=\"o\",\n",
    "                color=tue_col_2,\n",
    "                s=40,\n",
    "                label=\"Sensor Locations\",\n",
    "            )\n",
    "        elif cond_name == \"BoundaryObservation\":\n",
    "            ax_u.errorbar(\n",
    "                cond.X,\n",
    "                cond.y_vec,\n",
    "                yerr=np.sqrt(cond.op.sigma2),\n",
    "                fmt=\"o\",\n",
    "                color=tue_col_2,\n",
    "                label=\"Dirichlet Condition\",\n",
    "                zorder=5,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: Unknown condition type for plotting: {cond_name}\")\n",
    "\n",
    "    ax_u.legend(loc=\"lower left\")\n",
    "    ax_pde.legend(loc=\"upper right\")\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install tueplots\n",
    "!pip install jaxtyping\n",
    "!pip install beartype\n",
    "!pip install IPython\n",
    "!pip install ipykernel\n",
    "!pip install ipywidgets\n",
    "!pip install ipympl\n",
    "!pip install plum-dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Gaussian Processes to Physics-Informed Regression\n",
    "\n",
    "Welcome! This tutorial will let you implement Gaussian Processes (GPs) regression from scratch and demonstrate how to embed physical laws into them in different ways. We will cover:\n",
    "\n",
    "1.  **Part 1: A Primer on Gaussian Processes:** We will start with the fundamentals of GP regression, understanding them as distributions over functions and using them for interpolating data.\n",
    "\n",
    "2.  **Part 2: Physics-Informed Regression:** We will then see how to condition a Gaussian Process on domain knowledge arising in physical applications, such as boundary conditions or the PDE itself.\n",
    "\n",
    "By the end, you will see how this approach allows us to make accurate predictions even with very sparse data, which is a common problem in many scientific and engineering applications. Our goal is to build a solid intuition for these powerful techniques and gain understanding on how to implement such concepts in a general way.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Tools We'll Use\n",
    "\n",
    "We'll be using a modern, powerful stack of libraries for this tutorial.\n",
    "\n",
    "* **JAX**: A high-performance numerical computing library from Google. We use **JAX** for its NumPy-like API, automatic differentiation capabilities, and just-in-time (JIT) compilations. We'll enable 64-bit precision (double) for better numerical stability in our linear algebra operations.\n",
    "\n",
    "* **Jaxtyping & Beartype**: To write bug-free code, we'll heavily rely on type annotations. **Jaxtyping** allows us to annotate the shapes and data types of our JAX arrays directly in the function signatures. **Beartype** then acts as a runtime type-checker, catching shape-related errors as soon as they happen, which is invaluable for debugging.\n",
    "\n",
    "* **Plum & Multiple Dispatch**: We will use `plum-dispatch` to implement multiple dispatch. This is a powerful programming paradigm that allows us to define multiple versions of the same function that operate on different data types. It helps in writing clean and extensible code by avoiding complex `if/else` chains.\n",
    "\n",
    "* **Tueplots**: For all our visualizations, we will use **Tueplots**, a library designed to create aesthetically pleasing, publication-quality plots with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp # API compatible with numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # for matplotlib compatibility\n",
    "from tueplots import bundles\n",
    "\n",
    "# JAX settings - enable 64-bit precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# Suppress warnings for a cleaner notebook\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %% Jaxtyping and Beartype\n",
    "# Enable runtime type checking to catch shape errors\n",
    "%load_ext jaxtyping\n",
    "%jaxtyping.typechecker beartype.beartype\n",
    "\n",
    "# %% Plot settings - enable latex packages\n",
    "rcparams = bundles.beamer_moml()\n",
    "rcparams[\"text.latex.preamble\"] = r\"\\usepackage{amsfonts}\\n\\usepackage{siunitx}\\n\\usepackage{bm}\"\n",
    "plt.rcParams.update(rcparams)\n",
    "\n",
    "# %% Utility functions\n",
    "def atleast_2d(arr: jax.Array) -> jax.Array:\n",
    "    arr = jnp.asarray(arr)\n",
    "    if arr.ndim == 1:\n",
    "        return arr[:, None]\n",
    "    if arr.ndim == 2:\n",
    "        return arr\n",
    "    msg = f\"Input array must be 1D or 2D, but got {arr.ndim}D.\"\n",
    "    raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Quick Example of `jaxtyping`\n",
    "\n",
    "To understand how `jaxtyping` and `beartype` help us, let's look at a quick example. Imagine we have a function that performs a matrix-vector multiplication.\n",
    "\n",
    "With `jaxtyping`, we can write the function signature like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Float\n",
    "\n",
    "def matrix_vector_product(\n",
    "    matrix: Float[Array, \"rows cols\"], \n",
    "    vector: Float[Array, \" cols\"]\n",
    ") -> Float[Array, \" rows\"]:\n",
    "    return matrix @ vector\n",
    "\n",
    "A = jnp.ones((3, 2))  # Shape is (rows=3, cols=2)\n",
    "x = jnp.ones(2)       # Shape is (cols=2)\n",
    "result = matrix_vector_product(A, x)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When things go wrong, `beartype` catches the error immediately because the shapes don't match the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will raise an error\n",
    "A = jnp.ones((3, 2))  # Shape is (rows=3, cols=2)\n",
    "y = jnp.ones(5)       # Shape is (5), but expected (cols=2)\n",
    "try:\n",
    "    matrix_vector_product(A, y)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: A primer on Gaussian Processes (from an implementation perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Array, Float\n",
    "from collections.abc import Callable\n",
    "from dataclasses import dataclass \n",
    "# We can use this to save writing __init__ for parameter definitions.\n",
    "\n",
    "FloatType = float  | Float[Array, \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part we will see the main idea behind GP regression, see how to implement GPs (as it is done in probnum, GPytorch and GPJax), play around with kernels and learn how to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 — Problem setup: Damped Oscillator (data for GP regression)\n",
    "\n",
    "**Goal.** Define a physical signal (damped harmonic oscillator), generate sparse/noisy observations, and visualize the true trajectory.  \n",
    "This will be the dataset for vanilla GP regression in Part 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Damped oscillator model\n",
    "\n",
    "We model the displacement $y(t)$ as\n",
    "$$ y(t) = A \\, e^{-\\gamma t} \\cos(\\omega t),$$\n",
    "with parameters $\\theta = (A, \\gamma, \\omega)$.\n",
    "\n",
    "Interpretation:\n",
    "- $A$: initial amplitude (units of displacement)\n",
    "- $\\gamma$: damping coefficient (1/time); (larger ⇒ faster decay) \n",
    "- $\\omega$: angular frequency (rad/time); (larger ⇒ faster oscillation)\n",
    "\n",
    "We keep this deterministic here. Stochasticity will enter via measurement noise when generating training data. Why this system? It is linear, underdamped, and has interpretable parameters. Also it has some clear structure which we will later exploit for learning about including prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna start our tutorial by collecting some data about a physical system. Consider the following damped oscillator simulation. We collect its discplacement at 15 observation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DampedOscillatorParams:\n",
    "    \"\"\"Physical parameters for the damped oscillator.\"\"\"\n",
    "    A: FloatType = 1.0      # Amplitude\n",
    "    gamma: FloatType = 0.25 # Damping coefficient\n",
    "    omega: FloatType = 2.0  # Angular frequency\n",
    "\n",
    "def damped_oscillator(\n",
    "    t: Float[Array, \" T\"],\n",
    "    params: DampedOscillatorParams\n",
    ") -> Float[Array, \" T\"]:\n",
    "    \"\"\"\n",
    "    Displacement y(t) = A * exp(-gamma * t) * cos(omega *t)\n",
    "\n",
    "    Args:\n",
    "        t: scalar or array of times.\n",
    "        params: (A, gamma, omega).\n",
    "\n",
    "    Returns:\n",
    "        y(t) with the same shape as t.\n",
    "    \"\"\"\n",
    "    A, gamma, omega = params.A, params.gamma, params.omega\n",
    "    return A * jnp.exp(-gamma * t) * jnp.cos(omega * t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us animate this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter1 import animate_damped_oscillator\n",
    "\n",
    "# Time points for animation\n",
    "t_anim = jnp.linspace(0, 20, 200)\n",
    "\n",
    "# \"true\" physical parameters for data generation\n",
    "theta_true = DampedOscillatorParams(A=1.0, gamma=0.25, omega=2.0)\n",
    "\n",
    "animate_damped_oscillator(\n",
    "    damped_oscillator,\n",
    "    t_anim,\n",
    "    theta_true\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test data\n",
    "\n",
    "We define a **dense test grid** for plotting the true trajectory. We will later sample **sparse, noisy observations** for training. For reproducibility we will use the **JAX PRNG** with a fixed seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibility\n",
    "key = jax.random.key(42)\n",
    "\n",
    "# time ranges\n",
    "t_min, t_max = 0.0, 10.0\n",
    "X_test = jnp.linspace(t_min, t_max, 600) # for smooth plots\n",
    "\n",
    "y_test = damped_oscillator(X_test, theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data\n",
    "\n",
    "We simulate a typical experimental setting:\n",
    "\n",
    "- Sample $n_{\\text{train}}$ time points uniformly in $[t_{\\min}, t_{\\max}]$.\n",
    "- Observe $y_i = y(t_i) + \\varepsilon_i$ with i.i.d. Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "**Parameters**\n",
    "- `n_train`: number of observations\n",
    "- `sigma_noise`: measurement noise std (controls signal-to-noise ratio).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import PRNGKeyArray\n",
    "\n",
    "def make_training_data(\n",
    "    key: PRNGKeyArray,\n",
    "    n_train: int,\n",
    "    t_min: float,\n",
    "    t_max: float,\n",
    "    params: DampedOscillatorParams,\n",
    "    sigma_noise: float = 0.1,\n",
    ") -> tuple[Float[Array, \"N 1\"], Float[Array, \" N\"]]:\n",
    "    \"\"\"\n",
    "    Create sparse, noisy observations (X_train, y_train).count\n",
    "    \"\"\"\n",
    "    k1, k2 = jax.random.split(key)\n",
    "    X = jax.random.uniform(k1, shape=(n_train, 1), minval=t_min, maxval=t_max)\n",
    "    y_clean = damped_oscillator(X.squeeze(-1), params)\n",
    "    y_noisy = y_clean + sigma_noise * jax.random.normal(k2, shape=y_clean.shape)\n",
    "    return X, y_noisy\n",
    "\n",
    "n_train = 30\n",
    "sigma_noise = 0.05\n",
    "X_train, y_train = make_training_data(\n",
    "    key, n_train=n_train, t_min=t_min, t_max=t_max, params=theta_true, sigma_noise=sigma_noise\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now inspect the dataset.\n",
    "\n",
    "- Red line: true (noise-free) trajectory on a dense grid.\n",
    "- Blue crosses: sparse, noisy training points that the GP will fit in the the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.asarray(X_test), np.asarray(y_test), label=\"True signal\", linewidth=2)\n",
    "ax.scatter(np.asarray(X_train.squeeze(-1)), np.asarray(y_train), marker=\"x\", s=60, label=\"Training data\", color=\"blue\")\n",
    "ax.set_xlabel(\"time t\")\n",
    "ax.set_ylabel(\"displacement y(t)\")\n",
    "ax.set_title(\"Damped Oscillator — Data for GP Regression\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Processes (GPs)\n",
    "\n",
    "A **Gaussian process** is a powerful tool that lets us define a *distribution over functions*. When we say a function $f$ is drawn from a GP, we write:\n",
    "\n",
    "$$f \\sim \\mathcal{GP}(m, k)$$\n",
    "\n",
    "where $m$ is the **mean function** and $k$ is the **kernel function**.\n",
    "\n",
    "This means that if you pick any finite set of points, say $X = [x_1, \\dots, x_n]$, the function values at those points, $f(X)$, will follow a multivariate normal distribution:\n",
    "\n",
    "$$ f(X) \\sim \\mathcal{N}\\big(m(X),\\, K(X, X) \\big) $$\n",
    "\n",
    "The mean vector is given by $m(X) = [m(x_i)]_i$, and the covariance matrix, often called the **Gram matrix**, is constructed from the kernel function: $K_{ij} = k(x_i, x_j)$.\n",
    "\n",
    "#### The Kernel Function \n",
    "\n",
    "The **kernel function** is the most important ingredient. It encodes our assumptions about the function we are trying to model (e.g., is it smooth? periodic? rough?). A kernel takes two points, $x$ and $x'$, and returns a scalar value that represents their \"similarity\".\n",
    "\n",
    "* **High kernel value**: The points are considered \"similar,\" and the function values at these points are expected to be strongly correlated.\n",
    "\n",
    "* **Low kernel value**: The points are \"dissimilar,\" and their function values are less correlated.\n",
    "\n",
    "Any function can be a kernel as long as the resulting Gram matrix is symmetric and positive semi-definite. Here are some popular choices:\n",
    "\n",
    "**1. Radial Basis Function (RBF) Kernel**\n",
    "\n",
    "This is the most common kernel, often called the \"squared exponential\" kernel. It's a great default choice and assumes the function is infinitely differentiable (i.e., very smooth).\n",
    "\n",
    "$$ k_\\mathrm{RBF}(x, x') = \\sigma^2 \\exp \\left( - \\frac{\\| x - x' \\|^2}{2\\ell^2} \\right) $$\n",
    "\n",
    "* `variance` ($\\sigma^2$): Controls the average vertical variation of the function.\n",
    "* `lengthscale` ($\\ell$): Controls the horizontal \"wiggliness\". A small $\\ell$ leads to functions that vary quickly, while a large $\\ell$ leads to smoother, slowly varying functions.\n",
    "\n",
    "\n",
    "**2. Matérn 5/2 Kernel**\n",
    "\n",
    "The Matérn family of kernels is a generalization of the RBF kernel. The Matérn 5/2 is a popular choice because it assumes the function is twice-differentiable, making it suitable for modeling physical processes that are not infinitely smooth.\n",
    "\n",
    "$$ k_{\\mathrm{Matérn}5/2}(r) = \\sigma^2 \\left( 1 + \\frac{\\sqrt{5}r}{\\ell} + \\frac{5r^2}{3\\ell^2} \\right) \\exp \\left( - \\frac{\\sqrt{5}r}{\\ell} \\right), \\quad \\text{where } r = \\| x - x' \\| $$\n",
    "\n",
    "\n",
    "**3. Periodic Kernel**\n",
    "This kernel is perfect for modeling functions that exhibit repetitive patterns, like seasonal data or wave-like signals. It constructs a periodic function by mapping the input space onto a circle using the sine function and then applying an RBF-like kernel in this new space.\n",
    "\n",
    "$$k_\\mathrm{Periodic}(x, x') = \\sigma^2 \\exp \\left( - \\frac{2 \\sin^2(\\pi \\|x - x'\\| / p)}{\\ell^2} \\right)$$\n",
    "\n",
    "It has three key hyperparameters:\n",
    "\n",
    "* **`variance` ($\\sigma^2$)**: Similar to the RBF kernel, this controls the average vertical variation of the function.\n",
    "* **`lengthscale` ($\\ell$)**: This controls the smoothness of the function *within a single period*. A small lengthscale will lead to more complex, wiggly patterns inside each repetition.\n",
    "* **`period` ($p$)**: This is the most important parameter here, as it defines the distance over which the function repeats itself.\n",
    "\n",
    "This kernel is a great choice when you have a strong prior belief that your underlying function is periodic.\n",
    "\n",
    "#### What we need to implement\n",
    "\n",
    "We will implement a Gaussian Process from the ground up. This will help solidify our understanding of how they work and what is needed. Our implementation will be modular, consisting of four main components:\n",
    "\n",
    "1. **Gaussian**, a multivariate Gaussian distributed random variable, which we will use for sampling.\n",
    "\n",
    "2.  **Mean Functions (`m(x)`)**: These define the average value of the function. For simplicity, we'll start with a `ZeroFunction`, which assumes the function's mean is zero everywhere.\n",
    "\n",
    "2.  **Kernel Functions (`k(x, x')`)**: This is the heart of the GP. The kernel, also known as a covariance function, defines the \"similarity\" between points. It encodes our assumptions about the function's smoothness and shape.\n",
    "\n",
    "3.  **The `GaussianProcess` Class**: This class will bring the mean and kernel functions together to define the GP prior distribution.\n",
    "\n",
    "For simplicity we will introduce a `MeanFunction` and `KernelFunction` Type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cached_property\n",
    "\n",
    "@dataclass\n",
    "class Gaussian:\n",
    "    mu: Float[Array, \" N\"]\n",
    "    Sigma: Float[Array, \"N N\"]\n",
    "    jitter: FloatType = 1e-9\n",
    "\n",
    "    @cached_property\n",
    "    def L(self) -> Float[Array, \"N N\"]:\n",
    "        \"\"\"Cholesky factor of the covariance matrix.\"\"\"\n",
    "        return jnp.linalg.cholesky(self.Sigma + self.jitter * jnp.eye(self.Sigma.shape[0]))\n",
    "\n",
    "    def sample(self, key: PRNGKeyArray, num_samples: int = 1) -> Float[Array, \"num_samples N\"]:\n",
    "        \"\"\"\n",
    "        Sample from the multivariate normal distribution N(mu, Sigma). \n",
    "\n",
    "        Hint: Use the reparameterization trick. Use the cholesky factor L of the covariance matrix Sigma to transform \n",
    "        standard normal samples z ~ N(0, I) into samples y ~ N(mu, Sigma).\n",
    "        \"\"\" \n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"You need to implement the sample method!\")\n",
    "    \n",
    "    def log_pdf(self, x: Float[Array, \" N\"]) -> Float[Array, \"\"]:\n",
    "        \"\"\"\n",
    "        Computes the log porbability density of a vector x under the multivariate normal distribution N(mu, Sigma).\n",
    "\n",
    "        The formula for the log PDF is:\n",
    "            log p(x) = -0.5 * [ (x - mu)^T Sigma^{-1} (x - mu) + log|Sigma| + N log(2*pi) ]\n",
    "        \"\"\"\n",
    "        x = jnp.atleast_1d(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Step 1: Calculate the quadratic term: (x - mu)^T * Sigma^{-1} * (x - mu)\n",
    "        # Compute difference from the mean\n",
    "        resid = x - self.mu\n",
    "\n",
    "        # Solve for v in L * v = resid, which is equivalent to v = L^{-1} @ resid\n",
    "        v = jax.scipy.linalg.solve_triangular(self.L, resid, lower=True)\n",
    "\n",
    "        # Compute quadratic term v.T @ v\n",
    "        quad = jnp.dot(v, v)\n",
    "\n",
    "        # Step 2: Calculate the log determinant term: log|Sigma|\n",
    "        logdet = 2.0 * jnp.sum(jnp.log(jnp.diag(self.L))) \n",
    "\n",
    "        # Step 3: Combine terms\n",
    "        return -0.5 * (quad + logdet + n * jnp.log(2.0 * jnp.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR CODE\n",
    "# 1. Define a simple 2D Gaussian distribution\n",
    "true_mean = jnp.array([1.0, -2.0])\n",
    "true_cov = jnp.array([[2.0, 0.8], \n",
    "                     [0.8, 1.0]])\n",
    "gaussian = Gaussian(mu=true_mean, Sigma=true_cov)\n",
    "\n",
    "# 2. Draw a large number of samples\n",
    "key = jax.random.key(0)\n",
    "num_samples = 200_000\n",
    "samples = gaussian.sample(key, num_samples=num_samples)\n",
    "\n",
    "# 3. Compute the empirical mean and covariance\n",
    "empirical_mean = jnp.mean(samples, axis=0)\n",
    "empirical_cov = jnp.cov(samples, rowvar=False)\n",
    "\n",
    "# 4. Compare and print the results\n",
    "print(\"True Mean:\\n\", true_mean)\n",
    "print(\"Empirical Mean:\\n\", empirical_mean)\n",
    "print(\"\\nTrue Covariance:\\n\", true_cov)\n",
    "print(\"Empirical Covariance:\\n\", empirical_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanFunction = Callable[[Float[Array, \"N D\"]], Float[Array, \" N\"]]\n",
    "\n",
    "@dataclass\n",
    "class ZeroFunction:\n",
    "    def __call__(self, X: Float[Array, \"N D\"]) -> Float[Array, \" N\"]:\n",
    "        return jnp.zeros(X.shape[0])\n",
    "    \n",
    "@dataclass\n",
    "class ConstantFunction:\n",
    "    value: float = 0.0\n",
    "\n",
    "    def __call__(self, X: Float[Array, \"N D\"]) -> Float[Array, \" N\"]:\n",
    "        return jnp.full(X.shape[0], self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from dataclasses import fields\n",
    "\n",
    "KernelFunction = Callable[[Float[Array, \"N D\"], Float[Array, \"M D\"]], Float[Array, \"N M\"]]\n",
    "\n",
    "def _sqeuclidean(X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "    \"\"\"Pairwise squared Euclidean distances between two sets of vectors.\"\"\"\n",
    "    diff = X[:, None, :] - Y[None, :, :]  # (N, M, D)\n",
    "    return jnp.sum(diff**2, axis=-1)         # (N, M)\n",
    "\n",
    "@dataclass \n",
    "class Kernel:\n",
    "    \"\"\"A base class for all kernel functions.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_params(cls, params: dict[str, FloatType]) -> KernelFunction:\n",
    "        \"\"\"Constructs a kernel instance from a dictionary of parameters.\n",
    "        \n",
    "        This method automatically filters the dictionary to only use the parameters that \n",
    "        the specific kernel dataclass expects.\n",
    "        \"\"\"\n",
    "        valid_keys = {f.name for f in fields(cls)}\n",
    "        filtered_params = {k: v for k, v in params.items() if k in valid_keys}\n",
    "        return cls(**filtered_params)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        \"\"\"Compute the kernel matrix between two sets of input points X and Y.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RBF(Kernel):\n",
    "    variance: FloatType = 1.0\n",
    "    lengthscale: FloatType = 1.0\n",
    "\n",
    "    def __call__(self, X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        \"\"\"\n",
    "        Implement the RBF kernel.\n",
    "        k(x, x') = variance * exp(-0.5 * ||x - x'||^2 / lengthscale^2)\n",
    "\n",
    "        You can use the provided `_sqeuclidean` function to compute pairwise squared distances.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError(\"You need to implement the RBF kernel!\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Matern52(Kernel):\n",
    "    variance: FloatType = 1.0\n",
    "    lengthscale: FloatType = 1.0\n",
    "\n",
    "    def __call__(self, X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        d2 = _sqeuclidean(X, Y)\n",
    "        r = jnp.sqrt(jnp.maximum(d2, 0.0))\n",
    "        s = jnp.sqrt(5.0) * r / self.lengthscale\n",
    "        return self.variance * (1.0 + s + (s**2) / 3.0) * jnp.exp(-s)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Periodic(Kernel):\n",
    "    variance: FloatType = 1.0\n",
    "    lengthscale: FloatType = 1.0\n",
    "    period: FloatType = 2.0 * jnp.pi\n",
    "\n",
    "    def __call__(self, X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        d = jnp.abs(X[:, None, :] - Y[None, :, :])       # (N, M, D)\n",
    "        s = jnp.sin(jnp.pi * d / self.period)\n",
    "        d2 = jnp.sum(s * s, axis=-1)                     # (N, M)\n",
    "        return self.variance * jnp.exp(-2.0 * d2 / (self.lengthscale ** 2))\n",
    "    \n",
    "KERNEL_REGISTRY: dict[str, KernelFunction] = {\n",
    "    \"RBF\": RBF,\n",
    "    \"Matérn 5/2\": Matern52,\n",
    "    \"Periodic\": Periodic,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR CODE\n",
    "x_grid = jnp.linspace(-5, 5, 500)[:, None]\n",
    "kernel = RBF()\n",
    "values = kernel(x_grid, jnp.zeros((1, 1)))\n",
    "print(values.shape) # Should be of shape (500, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter1 import InteractiveGPPlotter\n",
    "\n",
    "plotter = InteractiveGPPlotter(\n",
    "    plot_type=\"kernel\",\n",
    "    kernel_registry=KERNEL_REGISTRY,\n",
    ")\n",
    "plotter.ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GaussianProcess:\n",
    "    \"\"\"\n",
    "    A GP prior is fully specified by its mean and kernel function.\n",
    "    m: (N, D) -> (N,)\n",
    "    k: (N, D), (M, D) -> (N, M)\n",
    "    \"\"\"\n",
    "    m: MeanFunction\n",
    "    k: KernelFunction\n",
    "\n",
    "    def __call__(self, X: Float[Array, \"N D\"]) -> \"Gaussian\":\n",
    "        \"\"\"\n",
    "        Evaluate the GP prior at a given set of (grid) points.\n",
    "        \n",
    "        This is the core of the GP. According to the definition, evaluating a GP at a set of points X\n",
    "        gives a multivariate normal distribution.\n",
    "\n",
    "        Task: Compute the mean and covariance.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError(\"You need to implement the __call__method!\")\n",
    "\n",
    "        # SOLUTION\n",
    "        mu = self.m(X)\n",
    "        Sigma = self.k(X, X)\n",
    "        return Gaussian(mu, Sigma)\n",
    "\n",
    "    def condition(\n",
    "        self,\n",
    "        y: Float[Array, \" N\"],\n",
    "        X: Float[Array, \"N D\"],\n",
    "        sigma2: float,\n",
    "    ) -> \"GaussianProcess\":\n",
    "        \"\"\"Conditions the GP on observed data. We will implement this class later.\"\"\"\n",
    "        return ConditionalGaussianProcess(self, y, X, sigma2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter1 import plot_gp\n",
    "\n",
    "# TEST YOUR CODE\n",
    "gp = GaussianProcess(m=ZeroFunction(), k=RBF(variance=1.0, lengthscale=1.0))\n",
    "X_grid = jnp.linspace(-5, 5, 500)[:, None]\n",
    "dist = gp(X_grid)\n",
    "print(f\"Mean shape: {dist.mu.shape}, Cov shape: {dist.Sigma.shape}\") # Should be shape (500,) and (500, 500)\n",
    "\n",
    "# ## TEST PLOT\n",
    "# fig= plt.figure(figsize=(9, 4.5))\n",
    "# ax = plt.gca()\n",
    "# plot_gp(ax, gp, X_grid, n_samples=5, seed=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented the core components, let us bring them to life! The widget below allows you to draw function samples directly from the Gaussian Process prior you define.\n",
    "\n",
    "**Things to Try**:\n",
    "- **Kernel**: Switch between the `RBF`, `Matern 5/2` and `Periodic` kernels. Notice the difference in smoothness?\n",
    "- `lengthscale`: How does a small vs. a large lengthscale affect the functions?\n",
    "- `variance`: What happens to the vertical spread of the functions when you change the variance?\n",
    "- `period`: What happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = InteractiveGPPlotter(\n",
    "    plot_type='prior',\n",
    "    kernel_registry=KERNEL_REGISTRY,\n",
    "    gp_cls=GaussianProcess,\n",
    "    mean_fn=ZeroFunction(),    \n",
    ")\n",
    "plotter.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deriving the GP Posterior: Conditioning on Data\n",
    "\n",
    "So far, we have a prior -- a distribution over functions before seeing any data. Now, we want to update this prior belief using our noisy observations\n",
    "$(X, y)$ to get a posterior distribution by conditioning the GP on this data. For GPs, this is elegant because it is just conditioning a big joint Gaussian distribution.  \n",
    "\n",
    "##### Setup\n",
    "We start with our GP prior, $f \\sim \\mathcal{GP}(m,k)$. We consider now our training inputs $X$ (where we have data) and our test inputs $X_*$ (where we want to predict). The function \n",
    "values at all these points, $f(X)$ and $f(X_*)$, are jointly Gaussian:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} f(X) \\\\ f(X_*) \\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} m(X) \\\\ m(X_*) \\end{bmatrix}, \\begin{bmatrix} K_{XX} & K_{X X_*} \\\\ K_{X_* X} & K_{X_* X_*} \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Here, $K_{AB}$ is the matrix of the kernel evaluated between all points in sets $A$ and $B$.\n",
    "\n",
    "##### Observation Model\n",
    "Usually, we do not observe the true function $f(X)$ directly. Instead, we see noisy version $y = f(X) + \\varepsilon$, where the noise $\\varepsilon$ is typically Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$. \n",
    "This means our observations $y$ are also Gaussian:\n",
    "\n",
    "$$ y \\sim \\mathcal{N}\\big( m(X), K_{XX} + \\sigma^2 I \\big).$$\n",
    "\n",
    "##### Joint Distribution of Data and Predictions\n",
    "\n",
    "Now we can write the joint distribution of what we've seen ($y$) and what we want to predict ($f_*$):\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N}\\!\\left(\n",
    "\\begin{bmatrix}\n",
    "m(X) \\\\\n",
    "m(X_*)\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "K_{XX} + \\sigma^2 I & K_{X X_*} \\\\\n",
    "K_{X_* X} & K_{X_* X_*}\n",
    "\\end{bmatrix}\n",
    "\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "##### Closed-form conditioning\n",
    "\n",
    "For any joint Gaussian, there's a standard formula to find the conditional distribution $p(b \\mid a)$. Applying this rule to our joint distribution gives us the posterior predictive distribution $p(f_* \\mid y)$.  \n",
    "\n",
    "The posterior distribution for $f_*$ at test points $X_*$ is a new Gaussian distribution with the following mean and covariance:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\mu_{\\text{post}}(X_*) &= m(X_*) + K_{X_*X}(K_{XX}+\\sigma^2 I)^{-1}(y - m(X)), \\\\\\\\[6pt]\n",
    "K_{\\text{post}}(X_*,X_*) &= K_{X_*X_*} - K_{X_*X}(K_{XX}+\\sigma^2 I)^{-1}K_{XX_*}.\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n",
    "\n",
    "##### Implementation\n",
    "\n",
    "As we learned before, we should avoid computing a matrix inverse directly. We can rewrite the update rules using a pre-computed vector $\\alpha$ (representer weights) and the \n",
    "Cholesky factor $L$ of the training covariance matrix $(K_{XX} + \\sigma^2 I)$. \n",
    "\n",
    "First, solve for a weight vector $\\alpha$:\n",
    "$$\n",
    "\\alpha = (K_{XX} + \\sigma^2 I)^{-1} (y - m(X)).\n",
    "$$\n",
    "Then, the posterior mean is simply a weighted combination of kernel similarities:\n",
    "$$\n",
    "\\mu_{\\text{post}}(X_*) = m(X_*) + K_{X_*X}\\,\\alpha.\n",
    "$$\n",
    "The posterior covariance can be computed similarly using efficient triangular solves with the Cholesky factor $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement a conditional Gaussian Process.\n",
    "\n",
    "TASK:\n",
    "- Tip: You can use `jax.scipy.linalg.solve_triangular` for computing...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConditionalGaussianProcess(GaussianProcess):\n",
    "    \"\"\"\n",
    "    Represents the GP posterior distribution after conditioning on data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        prior: GaussianProcess,\n",
    "        y: Float[Array, \" N\"],\n",
    "        X: Float[Array, \"N D\"],\n",
    "        sigma2: float,\n",
    "        jitter: float = 1e-9,\n",
    "    ):\n",
    "        self.prior = prior\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.sigma2 = sigma2\n",
    "        self.jitter = jitter\n",
    "\n",
    "        # Pre-compute shared qunatities for efficient prediction\n",
    "        Kxx = prior.k(self.X, self.X) + (sigma2 + jitter) * jnp.eye(self.X.shape[0])\n",
    "        self.L = jax.scipy.linalg.cholesky(Kxx, lower=True)\n",
    "        resid = self.y - prior.m(self.X)\n",
    "        self.alpha = jax.scipy.linalg.solve(Kxx, resid)\n",
    "\n",
    "        # Alternatively, we can do the two-step triangular solve for better numerical stability:\n",
    "        # v = jax.scipy.linalg.solve_triangular(self.L, resid, lower=True)\n",
    "        # self.alpha = jax.scipy.linalg.solve_triangular(self.L.T, v, lower=False)\n",
    "\n",
    "        def m_post(Xstar: Float[Array, \"Nstar D\"]) -> Float[Array, \" Nstar\"]:\n",
    "            \"\"\"\n",
    "            Calculate the posterior mean.\n",
    "            m_post(Xstar) = m(Xstar) + K(Xstar, X) @ alpha\n",
    "            \n",
    "            Your task:\n",
    "            Compute the posterior mean at new input locations Xstar.\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError(\"You need to implement the posterior mean fucntion!\")\n",
    "\n",
    "        def k_post(Xstar1: Float[Array, \"Nstar1 D\"], Xstar2: Float[Array, \"Nstar2 D\"] = None) -> Float[Array, \"Nstar1 Nstar2\"]:\n",
    "            \"\"\"\n",
    "            Calculate the posterior covariance.\n",
    "            K_post(A, B) = K(A, B) - K(A, X) @ (Kxx)^-1 @ K(X, B)\n",
    "\n",
    "            Your task:\n",
    "            1. Use jax.scipy.linalg.solve_triangular with self.L to solve for v = L^-1 @ K(X, B). \n",
    "            Then solve for w = (L^T)^-1 @ v. \n",
    "            2. Compute the final result using the formula.            \n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError(\"You need to implement the posterior covariance function!\")\n",
    "        \n",
    "        super().__init__(m=m_post, k=k_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR CODE\n",
    "\n",
    "# 1. Define a GP Prior\n",
    "gp_prior = GaussianProcess(m=ZeroFunction(), k=RBF())\n",
    "\n",
    "# 2. Create some noisy training data\n",
    "X_train_test = jnp.array([[-4.0], [-1.0], [2.5]])\n",
    "y_true_test = gp_prior(X_train_test).sample(jax.random.key(1), num_samples=1).flatten()\n",
    "noise = 0.1\n",
    "y_train_test = y_true_test + noise * jax.random.normal(jax.random.key(2), y_true_test.shape)\n",
    "\n",
    "# 3. Condition the prior on the data to get the posterior\n",
    "gp_posterior = gp_prior.condition(y=y_train_test, X=X_train_test, sigma2=noise**2)\n",
    "\n",
    "# 4. Let's check the output shapes on a grid\n",
    "X_grid = jnp.linspace(-5, 5, 500)[:, None]\n",
    "dist_post = gp_posterior(X_grid)\n",
    "print(f\"Posterior Mean shape: {dist_post.mu.shape}, Posterior Cov shape: {dist_post.Sigma.shape}\") # Should be (500,) and (500, 500)\n",
    "\n",
    "# # 5. Plot the posterior belief\n",
    "# fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "# plot_gp(\n",
    "#     ax,\n",
    "#     gp_posterior,\n",
    "#     X_grid,\n",
    "#     n_samples=5,\n",
    "#     seed=42,\n",
    "#     mean_label=\"Posterior Mean\",\n",
    "#     obs=(X_train_test, y_train_test)\n",
    "# )\n",
    "# ax.set_title(\"GP Posterior after Conditioning on Data\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Good is Our Model?\n",
    "\n",
    "Before we start exploring the posterior, let us think about how to measure success. A plot gives us a good feeling, but a number is better for comparing different models. To do\n",
    "so we want to introduce two useful metrics to tell us quantitatively how well our GP is doing. This will be crucial for tuning hyperparameters or kernel choices later on.\n",
    "\n",
    "We will look at two key metrics: one for **accuracy** and one for **quality of uncertainty**.\n",
    "\n",
    "1. **Root Mean Squared Error (RMSE)**: The most common way to measure the average prediction error is the **RMSE**. It tells you on average how far off the model's mean prediction is from the true value. Of course, lower is better. $$ \\text{RMSE} \\;=\\; \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\big( \\hat{y}_i - y_i \\big)^2 }. $$\n",
    "2. **Mean Log Predictive Density (MLPD)**: With GPs we have a whole predictive distribution for each point. The **MLPD** measures how well this distribution explains the data we actually saw. We then average the log of the pointwise probabilities. A higher MLPD is better. The metric rewards a model that correctly expresses high uncertainty when it is not sure. $$ \\text{MLPD} \\;=\\; \\frac{1}{n} \\sum_{i=1}^n \\log \\mathcal{N}\\!\\big(y_i \\,;\\, \\mu_i, \\sigma_i^2\\big). $$\n",
    "\n",
    "In the next cell, you will see both of these metrics in action when conditioning on the training data specified at the beginning of the tutorial. Try to find the settings that give you the best RMSE and the best MLPD. Are they always the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = InteractiveGPPlotter(\n",
    "    plot_type='posterior',\n",
    "    kernel_registry=KERNEL_REGISTRY,\n",
    "    gp_cls=GaussianProcess,\n",
    "    mean_fn=ZeroFunction(),    \n",
    "    training_data=(X_train, y_train),\n",
    "    test_data=(X_test, y_test)\n",
    ")\n",
    "plotter.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Best Hyperparameters with the Marginal Log-Likelihood\n",
    "\n",
    "So far, we have been moving sliders to tune our GP's hyperaparameters (`lengthscale`, `variance`, `sigma_noise`). It is a good way to build intuition, but of course it is not practical. How can we find the best values automatically? \n",
    "A powerful principle is to choose the hyperparameters that makes our observed data most plausible/likely.\n",
    "\n",
    "##### The Marginal Log-Likelihood (MLL)\n",
    "\n",
    "The quantity we want to maximize is the Marginal Log-Likelihood (MLL). It is the answer to the question: Under our GP model with a given set of hyperparameters $\\theta$, what was the probability of seeing the exact training data $y$ that we observed? \n",
    "The term \"marginal\" ise used because we do not care about the specific underlying latent function $f$. We integrate it out of the picture to the marginal probability of the data $y$. $$ p(y \\mid X, \\theta) = \\int p(y \\mid f)\\, p(f \\mid X,\\theta)\\, df. $$\n",
    "\n",
    "For a GP, this probability has a convenient formula. We work with its logarithm for numerical stability and mathematical convenience:\n",
    "$$\n",
    "\\underbrace{\\log p(y \\mid X, \\theta)}_{\\text{Log-Likelihood}}\n",
    "= \\underbrace{-\\tfrac{1}{2}(y-m)^T (K_\\theta + \\sigma^2 I)^{-1} (y-m)}_{\\text{Data Fit Term}}\n",
    "\\;+\\; \\underbrace{-\\tfrac{1}{2}\\log \\text{det} \\big( K_\\theta + \\sigma^2 I \\big)}_{\\text{Complexity Penalty}}\n",
    "\\;+\\; \\underbrace{-\\tfrac{n}{2}\\log 2\\pi}_{\\text{Constant}}.\n",
    "$$\n",
    "\n",
    "Maximizing the MLL involves a trade-off between two competing terms, which perfectly embodies the principle of Occam's Razor: \n",
    "\n",
    "1. **Data Fit Term (Goodness-of-Fit)** This term is just what you would expect: it gets better (less negative) when the model's predictions are closer to the actual data points. If this were the only term, the model would overfit wildly to match the data perfectly.\n",
    "\n",
    "2. **Complexity Penalty** This term prevents overfitting. It penalizes models that are too complex. A model with a small `lengthscale`, for example, is very \"complex\" because it can wiggle a lot to fit any data. The GP prior says such functions are inherently less likely, and the log term reflects this by becoming large, which penalizes the overall MLL.\n",
    "\n",
    "The MLL is maximized for the simplest model that still explains the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def untyped(func):\n",
    "    \"\"\"Return undecorated original function if beartype wrapped it.\"\"\"\n",
    "    return getattr(func, '__beartype_func', func)\n",
    "\n",
    "class HyperparameterTuner:\n",
    "    \"\"\"Handles the logic of optimizing GP hyperparameters.\"\"\"\n",
    "\n",
    "    def __init__(self, X_train: jax.Array, y_train: jax.Array):\n",
    "        self.X = jnp.asarray(X_train, dtype=jnp.float64)\n",
    "        self.y = jnp.asarray(y_train, dtype=jnp.float64)\n",
    "\n",
    "    def _get_opt_keys(self, kernel_name: str, param_init: dict) -> list[str]:\n",
    "        \"\"\"Determines which parameters to optimize for a given kernel.\"\"\"\n",
    "        base_params = param_init.get(kernel_name, {})\n",
    "        return list(base_params.keys()) + [\"log_noise\"]\n",
    "\n",
    "    def _pack_params(self, params_dict: dict, opt_keys: list[str]) -> np.ndarray:\n",
    "        \"\"\"Packs a dictionary of parameters into a NumPy array for the optimizer.\"\"\"\n",
    "        return np.array([params_dict.get(k, 0.0) for k in opt_keys], dtype=np.float64)\n",
    "\n",
    "    def _unpack_params(self, theta: np.ndarray | jax.Array, opt_keys: list[str]) -> dict:\n",
    "        \"\"\"Unpacks an array from the optimizer into a dictionary.\"\"\"\n",
    "        return {key: val for key, val in zip(opt_keys, theta)}\n",
    "    \n",
    "    def _make_mll_objective(self, kernel_cls: type[Kernel], opt_keys: list[str]):\n",
    "        \"\"\"Creates the negative MLL function for the optimizer.\"\"\"\n",
    "        @jax.jit\n",
    "        def neg_mll(theta_values: jax.Array) -> jax.Array:\n",
    "            # 1. Unpack and transform parameters\n",
    "            params = self._unpack_params(theta_values, opt_keys)\n",
    "            log_noise = params.pop(\"log_noise\")\n",
    "            sigma2 = jnp.exp(2.0 * log_noise)\n",
    "            kernel_params = {\n",
    "                k[4:] if k.startswith(\"log_\") else k: jnp.exp(v)\n",
    "                if k.startswith(\"log_\") else v \n",
    "                for k, v in params.items()\n",
    "            }\n",
    "            kernel = kernel_cls.from_params(kernel_params)\n",
    "        \n",
    "            # TODO: Implement the MLL calculation using the steps outlined below.\n",
    "            NotImplementedError()\n",
    "            \n",
    "            # 3. Define the GP prior (using GaussianProcess class)\n",
    "            \n",
    "            # 4. Evaluate the prior at the training points. \n",
    "\n",
    "            # 5. The distribution of y is N(mu, K_theta + sigma^2 * I) - use log_pdf\n",
    "            mll = ...\n",
    "            return -mll\n",
    "\n",
    "        return jax.value_and_grad(neg_mll)\n",
    "\n",
    "    def fit(self, kernel_name: str, kernel_registry: dict, param_init_registry: dict) -> dict:\n",
    "        \"\"\"Performs the optimization for a given kernel.\"\"\"\n",
    "        \n",
    "        # 1. Look up the specific kernel class and params from the registries\n",
    "        kernel_cls = kernel_registry[kernel_name]\n",
    "        base_params = param_init_registry.get(kernel_name, {}).copy()\n",
    "        \n",
    "        # 2. Setup initial parameters and optimization keys\n",
    "        base_params.setdefault(\"log_noise\", jnp.log(0.1 * jnp.std(self.y)))\n",
    "        opt_keys = self._get_opt_keys(kernel_name, param_init_registry)\n",
    "        theta0 = self._pack_params(base_params, opt_keys)\n",
    "\n",
    "        # 3. Create the objective function and its gradient\n",
    "        mll_value_and_grad = self._make_mll_objective(kernel_cls, opt_keys)\n",
    "\n",
    "        def objective_for_scipy(theta: np.ndarray):\n",
    "            val, grad = mll_value_and_grad(jnp.asarray(theta))\n",
    "            return np.asarray(val, dtype=np.float64), np.asarray(grad, dtype=np.float64)\n",
    "\n",
    "        # 4. Run the optimizer\n",
    "        res = minimize(fun=objective_for_scipy, x0=theta0, jac=True, method=\"L-BFGS-B\")\n",
    "        \n",
    "        # 5. Return results (this part is the same)\n",
    "        optimized_params_log = self._unpack_params(res.x, opt_keys)\n",
    "        final_params = {}\n",
    "        for key, value in optimized_params_log.items():\n",
    "            if key.startswith(\"log_\"):\n",
    "                final_params[key[4:]] = np.exp(value)\n",
    "            else:\n",
    "                final_params[key] = value\n",
    "        final_params['noise'] = np.exp(optimized_params_log['log_noise'])\n",
    "        final_params['sigma2'] = final_params['noise']**2\n",
    "\n",
    "        return {\n",
    "            \"result\": res,\n",
    "            \"optimized_params_log\": optimized_params_log,\n",
    "            \"final_params\": final_params,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter1 import OptimizationWidget\n",
    "\n",
    "# Parameter initialization\n",
    "PARAMETER_INIT = {\n",
    "    \"RBF\": {\"log_variance\": 0.0, \"log_lengthscale\": 0.0},\n",
    "    \"Matérn 5/2\": {\"log_variance\": 0.0, \"log_lengthscale\": 0.0},\n",
    "    \"Periodic\": {\"log_variance\": 0.0, \"log_lengthscale\": 0.0, \"period\": np.pi * 2},\n",
    "}\n",
    "\n",
    "# 2. Create an instance of the tuner with your training data\n",
    "n_obs = 10\n",
    "tuner = HyperparameterTuner(X_train=X_train[:n_obs, :], y_train=y_train[:n_obs])\n",
    "\n",
    "# 3. Create an instance of the widget, injecting all dependencies\n",
    "optimization_widget = OptimizationWidget(\n",
    "    tuner=tuner,\n",
    "    kernel_registry=KERNEL_REGISTRY,\n",
    "    param_init_registry=PARAMETER_INIT,\n",
    "    test_data=(X_test, y_test),\n",
    "    extrapolate=False,\n",
    "    gp_cls=GaussianProcess,\n",
    ")\n",
    "\n",
    "# 4. Display the widget. This MUST be the last line in the cell.\n",
    "optimization_widget.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design Your Own Kernel!\n",
    "\n",
    "Now it is your turn to design a kernel. You can design custom kernels that capture complex, real-world structures by combining simpler ones. This is your chance to get creative and see if you can build a kernel that better explains our damped oscillator data. Your goal is to achieve the lowest possible Negative Marginal Log-Likelihood (MLL). The person with the best MLL wins!\n",
    "\n",
    "#### Some helpful rules of Kernel Algebra.\n",
    "\n",
    "Just like numbers, kernels have an algebra. If you have two valid kernels, $k_1$ and $k_2$, you can combine them in the following ways to create a new, valid kernel:\n",
    "\n",
    "- **Addition:** The sum of two kernels is also a valid kernel: $$k_{\\text{sum}}(x, x') = k_1(x, x') + k_2(x, x')\\,.$$\n",
    "- **Multiplication:** The product of two kernels is also a valid kernel: $$k_{\\text{prod}} = k_1(x, x') * k_2(x, x')\\,$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINE YOUR CUSTOM KERNEL\n",
    "@dataclass\n",
    "class LinearKernel(Kernel):\n",
    "    \"\"\"\n",
    "    A simple linear kernel: k(x, y) = variance * x^T y.\n",
    "    This kernel assumes the function is a straight line through the origin.\n",
    "    \"\"\"\n",
    "    variance: FloatType = 1.0\n",
    "    \n",
    "    def __call__(self, X: Float[Array, \"N D\"], Y: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        # For 1D inputs, this is equivalent to variance * x * y\n",
    "        return self.variance * (X @ Y.T)\n",
    "\n",
    "# 2. ADD YOUR KERNEL TO THE REGISTRIES\n",
    "KERNEL_REGISTRY[\"Linear\"] = LinearKernel\n",
    "PARAMETER_INIT[\"Linear\"] = {\n",
    "    \"log_variance\": jnp.log(1.0), \n",
    "    # Define the initial values. Putting `log_` prefix means we optimize in log-space.\n",
    "}\n",
    "\n",
    "# 3. RUN THE WIDGET\n",
    "tuner = HyperparameterTuner(X_train=X_train, y_train=y_train)\n",
    "widget = OptimizationWidget(\n",
    "    tuner=tuner,\n",
    "    kernel_registry=KERNEL_REGISTRY,\n",
    "    param_init_registry=PARAMETER_INIT,\n",
    "    test_data=(X_test, y_test),\n",
    "    gp_cls=GaussianProcess,\n",
    ")\n",
    "widget.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Physics-Informed Regression (1D Poisson with CPU toy model)\n",
    "\n",
    "#### Conditioning on Physics\n",
    "\n",
    "In Part 1, we crafted a special, physics-informed kernel from the analytical solution of an ODE. This is elegant, but what if an analytical solutions isn't available?\n",
    "\n",
    "In this chapter, we explore a more general and powerful technique: we start with a generic kernel (like Matern) that only assumes smoothness, and then we force its samples to obey the physics by directly conditioning the GP on the differential equation itself.\n",
    "\n",
    "The core idea: We will treat the PDE as a source of data. Instead of only having data points of the form `(location, temperature)`, we will create new, virtual data points of the form `(location, value of the PDE's right-hand-side)`. This allows us to inject our physical knowledge directly into the standard GP regression framework.\n",
    "\n",
    "#### The Toy Problem: 1D Heat Flow in a CPU\n",
    "\n",
    "We will model the staedy-state temperature distribution, $u(x)$ along a 1D slice of a CPU bar. The physics is governed by the Poisson equation, a cornerstone of heat transfer and electrostatics:\n",
    "\n",
    "$$ -\\kappa \\frac{d^2 u}{dx^2}(x) = \\dot q_V(x) $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $u(x)$ is the temperature. \n",
    "- $\\kappa$ is the material's thermal conductivity.\n",
    "- $-\\kappa \\tfrac{d^2u}{dx^2}$ represents the net heat leaving a point due to conduction. \n",
    "- $\\dot q_V(x)$ is the volumetric heat source (heat generated by CPU cores) and sink (heat lost to the environemnt).\n",
    "\n",
    "Why is this a good approach?\n",
    "\n",
    "By conditioning on the PDE, we:\n",
    "\n",
    "- in ject strong prior knowledge about the system's behavior.\n",
    "\n",
    "- achieve high data efficiency, requiring fewer physical sensors.\n",
    "\n",
    "- obtain physically consistent predictions, especially when extrapolating outside the data range.\n",
    "\n",
    "Let us see this in action. We will start with a generic prior and progressively add information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chapter2 import create_cpu_problem, plot_gp_belief_and_pde\n",
    "\n",
    "# Setup the toy problem\n",
    "problem = create_cpu_problem()\n",
    "\n",
    "# Define our prior belief\n",
    "prior_gp = GaussianProcess(\n",
    "    m=ConstantFunction(value=60.0), \n",
    "    k=Matern52(variance=3.0, lengthscale=0.75 * problem.width)\n",
    ")\n",
    "\n",
    "# Define a grid for plotting\n",
    "X_grid = np.linspace(0.0, problem.width, 400)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_gp_belief_and_pde(\n",
    "    gp=prior_gp,\n",
    "    problem=problem,\n",
    "    X_grid=X_grid,\n",
    "    conditions=None,  # No data conditions yet\n",
    "    n_samples=3,\n",
    "    title=\"Prior Belief vs. True CPU Physics\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear observations\n",
    "\n",
    "In \"vanilla\" GP regression, we condition on observations of the function itself. The key insight for physics-informed GPs is that if $L$ is a linear differential operator (like our $L = -\\kappa \\frac{d^2}{dx^2}$), we can treat $L_x(u)(x)$ as a new set of observations.\n",
    "\n",
    "Because a GP is closed under linear operations, if our prior belief about the temperature $u$ is a GP:\n",
    "\n",
    "$$ u(x) \\sim \\mathcal{GP}(m(x), k(x,x')) $$\n",
    "\n",
    "Then our belief about the operator image, $L_xu(x)$ is also a Gaussian Process!\n",
    "\n",
    "$$ (L_x u)(x) \\sim \\mathcal{GP}((L_xm)(x), (L_xL_{x'}k)(x, x')) $$\n",
    "\n",
    "This means we can use the standard rules of GP conditioning on this new, \"virtual\" data.\n",
    "\n",
    "\n",
    "#### Building the Joint Distribution\n",
    "\n",
    "The process is the same as for standard regression: we build a joint Gaussian distribution over what we want to predict (the temperature $u(X)$) and what we are observing (the PDE's value at a set of \"collocation points,\" $X_{\\text{PDE}}$). \n",
    "\n",
    "Our \"observation\" is that $(Lu)(X_{pde})$ should be equal to the known heat source $\\dot q_V(X_{pde})$. We can write this as a set of linear observations with some small noise tolerance $\\varepsilon$:\n",
    "$$\\dot{q}_V(X_{\\text{pde}}) = (L u)(X_{\\text{pde}}) + \\epsilon, \\qquad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2_{\\text{pde}} I)$$\n",
    "\n",
    "The joint distribution is:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "u(X) \\\\[2pt]\n",
    "(L u)(X_{\\text{pde}})\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\!\\left(\n",
    "\\begin{bmatrix}\n",
    "\\text{Prior Mean} \\\\\n",
    "\\text{Prior Mean of PDE}\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "\\text{Cov}(u, u) & \\text{Cov}(u, Lu) \\\\[2pt]\n",
    "\\text{Cov}(Lu, u) & \\text{Cov}(Lu, Lu)\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "This looks complex, but it's just the standard GP setup. Each block in the covariance matrix is derived from our prior kernel, $k(x, x')$, by applying the operator $L$ to its arguments. For our 1D heat equation ($L = -\\kappa \\frac{d^2}{dx^2}$):\n",
    "\n",
    "- $\\text{Cov}(u, u) = k(X, X)$\n",
    "- $\\text{Cov}(u, Lu) = (L_{x'}k)(X, X_{\\text{pde}}) = -\\kappa \\frac{\\partial^2}{\\partial {x'}^2} k(X, X_{\\text{pde}})$\n",
    "- $\\text{Cov}(Lu, Lu) = (L_x L_{x'}k)(X_{\\text{pde}}, X_{\\text{pde}}) = \\kappa^2 \\frac{\\partial^4}{\\partial x^2 \\partial {x'}^2} k(X_{\\text{pde}}, X_{\\text{pde}})$\n",
    "\n",
    "Once this joint distribution is built, we can apply the standard Gaussian conditioning formulas to get the posterior mean and covariance.\n",
    "\n",
    "\n",
    "#### Doing the Math: The Covariances for a Matérn-5/2 Kernel\n",
    "\n",
    "The theory is general, but to implement it, we need to compute the specific covariance terms like $\\text{Cov}(u, Lu)$ and $\\text{Cov}(Lu, Lu)$. This involves taking derivatives of our chosen prior kernel. Let's do this for a popular choice: the **Matérn-5/2 kernel**. Its twice-differentiable nature makes it a great candidate for modeling systems governed by second-order PDEs.\n",
    "\n",
    "We start with the 1D isotropic Matérn-5/2 kernel, which is a function of the distance $r = |x - y|$:\n",
    "$$k(x,y) = \\phi(r) = \\sigma^2\\left(1 + a r + \\frac{a^2 r^2}{3}\\right)e^{-a r}, \\quad \\text{where} \\quad a = \\frac{\\sqrt{5}}{\\ell}$$\n",
    "\n",
    "Our operator is $L = -\\kappa \\frac{d^2}{dx^2}$. To find the necessary covariance functions, we need to apply this operator to the kernel. In 1D, thanks to the chain rule, taking derivatives with respect to $x$ or $y$ simplifies to taking derivatives of $\\phi(r)$ with respect to $r$.\n",
    "\n",
    "The required derivatives of $\\phi(r)$ are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi''(r) &= \\frac{a^2\\sigma^2(a^2 r^2 - a r - 1)}{3}\\,e^{-a r} \\\\\n",
    "\\phi^{(4)}(r) &= \\frac{a^4\\sigma^2(a^2 r^2 - 5 a r + 3)}{3}\\,e^{-a r}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can construct the two key covariance blocks we need for our joint distribution:\n",
    "\n",
    "1.  **The Cross-Covariance:** This term, $\\text{Cov}(u(x), (Lu)(y))$, measures the correlation between the function at one point and the PDE's value at another.\n",
    "    $$\\text{Cov}(u, Lu) = L_y k(x,y) = -\\kappa \\frac{\\partial^2}{\\partial y^2}k(x,y) = -\\kappa\\,\\phi''(|x-y|)$$\n",
    "\n",
    "2.  **The PDE Covariance:** This term, $\\text{Cov}((Lu)(x), (Lu)(y))$, forms the Gram matrix for our \"PDE observations.\"\n",
    "    $$\\text{Cov}(Lu, Lu) = L_x L_y k(x,y) = \\kappa^2 \\frac{\\partial^4}{\\partial x^2 \\partial y^2}k(x,y) = \\kappa^2\\,\\phi^{(4)}(|x-y|)$$\n",
    "\n",
    "With these two closed-form expressions, we have everything we need to build the full covariance matrix for conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking It All Up: Combining All Your Knowledge\n",
    "\n",
    "In a real problem, we don't just have one source of information. We might have:\n",
    "- **Noisy sensor data:** $y_{\\text{data}} = u(X_{\\text{data}}) + \\epsilon$\n",
    "- **PDE constraints:** $y_{\\text{pde}} = (Lu)(X_{\\text{pde}}) + \\epsilon$\n",
    "- **Boundary conditions:** $y_{\\text{bc}} = u(X_{\\text{boundary}}) + \\epsilon$\n",
    "\n",
    "Each of these is a linear observation of our underlying function $u$. The most robust and stable way to combine them is to **stack them into a single, large linear system**. The core of GP conditioning is building a joint Gaussian distribution between what you want to predict and what you have observed. When we have multiple types of observations, we build a joint distribution over all of them.\n",
    "\n",
    "This results in a \"master\" covariance matrix that has a **block structure**:\n",
    "\n",
    "$$y_{\\text{all}} = \\begin{bmatrix} y_{\\text{data}} \\\\ y_{\\text{pde}} \\\\ y_{\\text{bc}} \\end{bmatrix}, \\qquad K_{YY, \\text{all}} = \\begin{bmatrix} K_{\\text{data,data}} & K_{\\text{data,pde}} & K_{\\text{data,bc}} \\\\ K_{\\text{pde,data}} & K_{\\text{pde,pde}} & K_{\\text{pde,bc}} \\\\ K_{\\text{bc,data}} & K_{\\text{bc,pde}} & K_{\\text{bc,bc}} \\end{bmatrix}$$\n",
    "\n",
    "- **Diagonal Blocks** (e.g., $K_{\\text{data}, \\text{data}}$): These are the standard Gram matrices you already know. They measure the internal correlation within a single type of observation (e.g., how two sensor readings relate to each other).\n",
    "\n",
    "- **Off-Diagonal Blocks** (e.g., $K_{\\text{data}, \\text{pde}}$): This is the new, crucial part! These are the cross-covariance matrices. They measure the relationship between different types of observations (e.g. how a sensor reading at $x_1$ relates to the PDE being satisfied at $x_2$).\n",
    "\n",
    "Then, we solve this single, larger system to get the final posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.scipy.linalg as jsl\n",
    "from plum import dispatch\n",
    "\n",
    "@dataclass\n",
    "class LinearObservation:\n",
    "    \"\"\"\n",
    "    A linear observation operator for f:\n",
    "    \n",
    "    Given (prior GP, X, y, [op params]), produce a linear block:\n",
    "        - mY: expected observation mean\n",
    "        - KYY_tilde: observation Gram + noise\n",
    "        - KfY_fn: function X* -> k(f(X^*), Y)\n",
    "    \n",
    "    The provided 'y' is returned back so the conditioner can stack it.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(\n",
    "        self,\n",
    "        prior,                      # GaussianProcess\n",
    "        X: Float[Array, \"N D\"],\n",
    "        y: Float[Array, \"N\"],\n",
    "    ) -> tuple[\n",
    "        Float[Array, \"N\"], \n",
    "        Float[Array, \"N N\"], \n",
    "        Callable[[Float[Array, \"M D\"]], Float[Array, \"N M\"]], \n",
    "        Float[Array, \"N\"]\n",
    "    ]:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "@dataclass\n",
    "class Block:\n",
    "    op: LinearObservation                \n",
    "    X: Float[Array, \"N D\"] | None        \n",
    "    y_vec: Float[Array, \"N\"] | None\n",
    "    mY: Float[Array, \"N\"] | None         \n",
    "    KYY_tilde: Float[Array, \"N N\"] | None\n",
    "    KfY_fn: Callable[[Float[Array, \"M D\"]], Float[Array, \"N M\"]] | None \n",
    "\n",
    "@dispatch\n",
    "def cross_covariance(op1: LinearObservation, op2: LinearObservation, prior_k: KernelFunction) -> KernelFunction:\n",
    "    msg = f\"cross-covariance not implemented for {type(op1)} and {type(op2)}\"\n",
    "    raise NotImplementedError(msg)\n",
    "\n",
    "@dataclass\n",
    "class LazyConditionalGaussianProcess:\n",
    "    prior: GaussianProcess\n",
    "    jitter: float = 1e-6\n",
    "    _blocks: list[Block] = None\n",
    "\n",
    "    def add_condition(self, X: Float[Array, \"N D\"], y: Float[Array, \"N\"], op: LinearObservation):\n",
    "        \"\"\"\n",
    "        Register a linear observation block defined by `op` at locations X with targets y.\n",
    "        Nothing is solved yet - we only collect blocks.\n",
    "        \"\"\"\n",
    "        mY, KYY_tilde, KfY_fn, y_vec = op(self.prior, X, y)\n",
    "        KYY_tilde = KYY_tilde + self.jitter * jnp.eye(KYY_tilde.shape[0], dtype=KYY_tilde.dtype)\n",
    "        \n",
    "        # Store condition\n",
    "        if not self._blocks:\n",
    "            self._blocks = []\n",
    "            \n",
    "        self._blocks.append(\n",
    "            Block(op=op, X=X, mY=mY, KYY_tilde=KYY_tilde, KfY_fn=KfY_fn, y_vec=y_vec)\n",
    "        )\n",
    "\n",
    "    def _compute_cross_covariance(self, block_i, block_j):\n",
    "        op_i, X_i = block_i.op, block_i.X \n",
    "        op_j, X_j = block_j.op, block_j.X\n",
    "\n",
    "        return cross_covariance(op_i, op_j, self.prior.k)(X_i, X_j)\n",
    "\n",
    "    def condition(self) -> GaussianProcess:\n",
    "        \"\"\"\n",
    "        Perform one joint conditioning step over all collected blocks and return a posterior\n",
    "        GaussianProcess with generic (non-stationary) kernel.\n",
    "        \"\"\"\n",
    "        if not self._blocks:\n",
    "            return self.prior\n",
    "        \n",
    "        # Stack means and observations\n",
    "        mY_all = jnp.concatenate([b.mY.reshape(-1) for b in self._blocks])\n",
    "        y_all = jnp.concatenate([b.y_vec.reshape(-1) for b in self._blocks])\n",
    "\n",
    "        # Build the full KYY matrix using the factory\n",
    "        num_blocks = len(self._blocks)\n",
    "        KYY_grid = [[None] * num_blocks for _ in range(num_blocks)]\n",
    "        for i in range(num_blocks):\n",
    "            for j in range(num_blocks):\n",
    "                if i == j:\n",
    "                    KYY_grid[i][j] = self._blocks[i].KYY_tilde\n",
    "                else:\n",
    "                    KYY_grid[i][j] = self._compute_cross_covariance(self._blocks[i], self._blocks[j])\n",
    "\n",
    "        KYY_all = jnp.block(KYY_grid)\n",
    "        KYY_all += self.jitter * jnp.eye(KYY_all.shape[0], dtype=KYY_all.dtype)\n",
    "        \n",
    "        # One Cholesky solve for all constraints\n",
    "        L = jsl.cholesky(KYY_all, lower=True)\n",
    "        resid = y_all - mY_all\n",
    "        v = jsl.solve_triangular(L, resid, lower=True)\n",
    "        alpha = jsl.solve_triangular(L.T, v, lower=False)\n",
    "\n",
    "        # Helper to horizontally stack cross-covariances from all blocks\n",
    "        def KfY_stacked(X: Float[Array, \"M D\"]) -> Float[Array, \"M N_total\"]:\n",
    "            parts = [fn(jnp.atleast_2d(X)) for fn in [b.KfY_fn for b in self._blocks]]\n",
    "            return jnp.concatenate(parts, axis=1)\n",
    "        \n",
    "        def m_post(Xstar: Float[Array, \"M D\"]) -> Float[Array, \" M\"]:\n",
    "            Xstar = jnp.atleast_2d(Xstar)\n",
    "            return self.prior.m(Xstar) + KfY_stacked(Xstar) @ alpha\n",
    "        \n",
    "        def _apply_KYY_inv(B: Float[Array, \"N_total M\"]) -> Float[Array, \"N_total M\"]:\n",
    "            v = jsl.solve_triangular(L, B, lower=True)\n",
    "            return jsl.solve_triangular(L.T, v, lower=False)\n",
    "\n",
    "        def k_post(Xa: Float[Array, \"M D\"], Xb: Float[Array, \"P D\"]) -> Float[Array, \"M P\"]:\n",
    "            Kxz = self.prior.k(Xa, Xb)\n",
    "            KxY = KfY_stacked(Xa)\n",
    "            KYz = KfY_stacked(Xb).T\n",
    "            return Kxz - KxY @ _apply_KYY_inv(KYz)\n",
    "\n",
    "        return GaussianProcess(\n",
    "            m=m_post,\n",
    "            k=k_post\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensor Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by creating our first and most fundamental brick: a block for standard sensor observations.\n",
    "\n",
    "We will call this block `SensorObservation`. It represents noisy, direct measurements of the temperature. It handles observations of the form:\n",
    "\n",
    "$$ y_{\\text{data}} = u(X_{\\text{data}}) + \\varepsilon, \\qquad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I) $$\n",
    "\n",
    "It needs to be able to compute three things:\n",
    "\n",
    "1. The prior mean of the observations: $$ \\mathbb{E}[u(X)] $$\n",
    "2. The prior covariance among the observations (its Gram matrix): $$\\text{Cov}(u(X), u(X))$$\n",
    "3. The cross-covariance between a new prediction point $X^*$ and the observations: $$ \\text{Cov}(u(X^*), u(X)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SensorObservation(LinearObservation):\n",
    "    r\"\"\"Noisy point observations: y = u(X) + eps, eps \\sim N(0, sigma^2 I).\"\"\"\n",
    "\n",
    "    sigma2: float\n",
    "\n",
    "    # This is a helper for the cross-covariance registy later.\n",
    "    def L_k(self, prior_k, X, X_other):\n",
    "        \"\"\"Computes cov(u(X), u(X_other)) = k(X, X_other).\"\"\"\n",
    "        return prior_k(X, X_other)\n",
    "    \n",
    "    def __call__(        \n",
    "        self, prior: KernelFunction, X: Float[Array, \"N 1\"], y: Float[Array, \" N\"],\n",
    "    ):\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # TODO: Implement the three key components for a standard GP observation.\n",
    "\n",
    "        # 1. Calculate `mY`, the prior mean at the observation points `X`. \n",
    "        # 2. Calculate `KYY_tilde`, the prior covariance at `X`, and remember to add the sensor noise variance `self.sigma2`.\n",
    "        # 3. Define `KfY_fn`, a function that takes new points `Xstar` and returns the cross-covariance between ``u(Xstar)`` and ``u(X)``.\n",
    "\n",
    "        mY = ...\n",
    "        KYY_tilde = ...\n",
    "\n",
    "        def KfY_fn(Xstar: Float[Array, \"Nstar 1\"]) -> Float[Array, \"Nstar N\"]:\n",
    "            return ...\n",
    "\n",
    "        raise NotImplementedError(\"You need to implement the SensorObservation operator!\")\n",
    "\n",
    "        return mY, KYY_tilde, KfY_fn, y\n",
    "\n",
    "# Here we provide the cross-covariance definition.\n",
    "# This tells the stacking engine how to compute the covariance\n",
    "# when two SensorObservation blocks interact (which is just the base kernel).\n",
    "@cross_covariance.dispatch\n",
    "def _(op1: SensorObservation, op2: SensorObservation, prior_k: KernelFunction) -> KernelFunction:\n",
    "    def k(X1: Float[Array, \"N D\"], X2: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        return op1.L_k(prior_k, X1, X2)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_gp = LazyConditionalGaussianProcess(prior=prior_gp, jitter=1e-9)\n",
    "\n",
    "# Access the synthetic sensor data from the object's attributes\n",
    "sensor_locations = problem.X_dts\n",
    "noisy_measurements = problem.y_dts\n",
    "noise_level_std = problem.dts_noise_std\n",
    "\n",
    "\n",
    "lazy_gp.add_condition(\n",
    "    X=jnp.asarray(sensor_locations).reshape(-1, 1),\n",
    "    y=jnp.asarray(noisy_measurements).reshape(-1),\n",
    "    op=SensorObservation(sigma2=noise_level_std**2)\n",
    ")\n",
    "\n",
    "post_gp = lazy_gp.condition()\n",
    "\n",
    "# Plot the posterior belief and PDE balance\n",
    "plot_gp_belief_and_pde(\n",
    "    gp=post_gp,\n",
    "    problem=problem,\n",
    "    X_grid=X_grid,\n",
    "    conditions=lazy_gp._blocks,\n",
    "    n_samples=3,\n",
    "    title=\"Posterior Belief after Conditioning on DTS Data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding PDE observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most important block: encoding the physics. The `PDEObservation` block represents our knowledge that the underlying temperature field $u(x)$ must satify the Poisson equation. It handles \"virtual\" observations of the form:\n",
    "\n",
    "$$ y_{\\text{PDE}} = (- \\kappa u'')(X_{\\text{pde}}) + \\varepsilon, \\qquad \\text{where } y_{\\text{pde}} \\text{ is the known heat source } \\dot q_V(X_{\\text{pde}}). $$\n",
    "\n",
    "To build this block, we need to compute the covariances that involve the linear operator $L= -\\kappa d^2/dx^2$. This is where the kernel derivaties we derived earlier come into play. This block will be responsible for calculating:\n",
    "\n",
    "1. The PDE-data cross-covariance: $$ \\text{Cov}(u(X), (Lu)(X_{\\text{pde}})) $$\n",
    "\n",
    "2. The PDE-PDE covariance: $$ \\text{Cov}((Lu)(X_\\text{pde})) $$\n",
    "\n",
    "Your task is to implement the methods that compute these two crucial components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PDEObservation(LinearObservation):\n",
    "    \"\"\"\n",
    "    Strong-form PDE collocation in 1D for L = -kappa d^2/dx^2 with a Matérn-5/2 prior.\n",
    "    \"\"\"\n",
    "    kappa: float\n",
    "    sigma2: float\n",
    "\n",
    "    def _check_if_matern52(self, prior_k: KernelFunction):\n",
    "        if not isinstance(prior_k, Matern52):\n",
    "            msg = f\"PDEObservation only supports Matérn-5/2 kernels, got {type(prior_k)}\"\n",
    "            raise ValueError(msg)\n",
    "\n",
    "    def L_k(self, prior_k: KernelFunction, X: Float[Array, \"Ndata D\"], X_pde: Float[Array, \"Npde D\"]) -> Float[Array, \"Ndata Npde\"]:\n",
    "        \"\"\"Computes cov(f(X), Lf(X_pde)) = ... \"\"\"\n",
    "\n",
    "        self._check_if_matern52(prior_k)        \n",
    "        variance, lengthscale = prior_k.variance, prior_k.lengthscale \n",
    "        \n",
    "        # --- YOUR CODE HERE ---\n",
    "        # TODO: Implement the formula for the cross-covariance cov(f(X), Lf\n",
    "        # (X_pde))\n",
    "        raise NotImplementedError(\"Implement the L_k method for PDEObservation\")\n",
    "\n",
    "    def L_k_L(self, prior_k: KernelFunction, X_pde1: Float[Array, \"NPDE1 D\"], X_pde2: Float[Array, \"NPDE2 D\"]) -> Float[Array, \"NPDE1 NPDE2\"]:\n",
    "        \"\"\"Computes cov(Lf(X_pde1), Lf(X_pde2)) = ... \"\"\"\n",
    "        self._check_if_matern52(prior_k)\n",
    "        variance, lengthscale = prior_k.variance, prior_k.lengthscale\n",
    "\n",
    "        # --- YOUR CODE HERE\n",
    "        # TODO: Implement the formula for the cross-covariance cov(Lf(X_pde1), Lf(X_pde2))\n",
    "        raise NotImplementedError(\"Implement the L_k_L method for PDEObservation\")\n",
    "\n",
    "    def __call__(\n",
    "            self, prior: KernelFunction, X: Float[Array, \"N D\"], y: Float[Array, \" N\"]\n",
    "        ) -> tuple[Float[Array, \" N\"], Float[Array, \"N N\"], Callable, Float[Array, \" N\"]]:\n",
    "        \n",
    "        # Mean: (-kappa) * m''(X_pde). m''=0 for a constant mean.\n",
    "        mY = jnp.zeros_like(y)\n",
    "        KYY_tilde = self.L_k_L(prior.k, X, X) + self.sigma2 * jnp.eye(X_pde.shape[0])\n",
    "\n",
    "        def KfY_fn(Xstar: Float[Array, \"Nstar D\"]) -> Float[Array, \"Nstar N\"]:\n",
    "            return self.L_k(prior.k, Xstar, X)\n",
    "        \n",
    "        return mY, KYY_tilde, KfY_fn, y\n",
    "\n",
    "\n",
    "@cross_covariance.dispatch\n",
    "def _(op1: PDEObservation, op2: SensorObservation, prior_k: KernelFunction) -> KernelFunction:\n",
    "    def k(X1: Float[Array, \"N D\"], X2: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        return op1.L_k(prior_k, X1, X2)\n",
    "    return k\n",
    "\n",
    "\n",
    "@cross_covariance.dispatch\n",
    "def _(op1: SensorObservation, op2: PDEObservation, prior_k: KernelFunction) -> KernelFunction:\n",
    "    def k(X1: Float[Array, \"N D\"], X2: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        return op2.L_k(prior_k, X2, X1).T\n",
    "    return k\n",
    "\n",
    "\n",
    "@cross_covariance.dispatch\n",
    "def _(op1: PDEObservation, op2: PDEObservation, prior_k: KernelFunction) -> KernelFunction:\n",
    "    def k(X1: Float[Array, \"N D\"], X2: Float[Array, \"M D\"]) -> Float[Array, \"N M\"]:\n",
    "        return op1.L_k_L(prior_k, X1, X2)\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDE collocation points using the problem's width\n",
    "X_pde = np.linspace(0.0, problem.width, 18, endpoint=False)[1:] # Avoid the boundary\n",
    "\n",
    "lazy_gp.add_condition(\n",
    "    X=jnp.asarray(X_pde).reshape(-1, 1),\n",
    "    y=jnp.asarray(problem.q_total(X_pde)),\n",
    "    op=PDEObservation(kappa=problem.kappa, sigma2=0.1**2)\n",
    ")\n",
    "\n",
    "# Compute the posterior GP after adding the PDE conditions\n",
    "post_gp = lazy_gp.condition()\n",
    "\n",
    "\n",
    "# Call the plotting function with the posterior GP\n",
    "# plot_conditions = create_plot_conditions_from_blocks(lazy_gp._blocks)\n",
    "plot_gp_belief_and_pde(\n",
    "    gp=post_gp,\n",
    "    problem=problem,\n",
    "    X_grid=X_grid,\n",
    "    conditions=lazy_gp._blocks,\n",
    "    n_samples=1,\n",
    "    seed=42,\n",
    "    title=\"GP Conditioned on DTS Data and PDE\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding boundary conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now conditioned our GP on sensor data and the underlying physics. If you look at the last plot, however, you will notice the uncertainty (the shaded blue region) is still quite large at the very edges of the domain. Our model is still unsure what the exact temperature is at $x = 0$ and $x = L$.\n",
    "\n",
    "The final piece is to add boundary conditions. A common type is the Dirichlet boundary condition, which directly specifies the value of the function at the boundary:\n",
    "\n",
    "$$ u(x) = c, \\qquad \\text{ for } x \\in \\delta \\Omega. $$\n",
    "\n",
    "In our 1D case, this means we specify the temperature at $x = 0$ and $x = L$. F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TODO: Implement the entire class `DirichletObservation` and visualize using the logic from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get boundary condition from the problem definition\n",
    "X_bc = jnp.asarray([problem.domain[0], problem.domain[1]]).reshape(-1, 1)\n",
    "y_bc = jnp.asarray(problem.solution(X_bc.ravel()))\n",
    "\n",
    "# Use a very small noise value to enforce the constraint strictly\n",
    "bc_noise_std = 1e-5\n",
    "bc_noise_var = bc_noise_std**2\n",
    "\n",
    "# Add Dirichlet Condition\n",
    "lazy_gp.add_condition(\n",
    "    X=X_bc,\n",
    "    y=y_bc,\n",
    "    op=DirichletObservation(sigma2=bc_noise_var)\n",
    ")\n",
    "\n",
    "# Solve for the new posterior and visualize\n",
    "post_gp_bc = lazy_gp.condition()\n",
    "\n",
    "# Visualize\n",
    "plot_gp_belief_and_pde(\n",
    "    gp=post_gp_bc,\n",
    "    problem=problem,\n",
    "    X_grid=X_grid,\n",
    "    n_samples=1,\n",
    "    conditions=lazy_gp._blocks,\n",
    "    title=\"Posterior Belief after Conditioning on Boundary Values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part rebuilds the framework behind the paper [\"Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers\"](arxiv.org/abs/2212.12474).\n",
    "\n",
    "The accompanying code basis contains a much more general framework (more kernels, more PDEs, more conditions) and a multiple of additional tutorials:\n",
    "\n",
    "https://github.com/marvinpfoertner/linpde-gp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gp_and_physics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
